\documentclass{article}

\usepackage[garamond, tableau]{jaspercommon}

\DeclareMathOperator{\RSK}{RSK}
\DeclareMathOperator{\RowInsert}{RowInsert}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\shape}{shape}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\type}{type}
\DeclareMathOperator{\Par}{Par}
\DeclareMathOperator{\SSYT}{SSYT}
\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Lex}{Lex}

\newcommand{\frkS}{\ensuremath\mathfrak{S}}
\newcommand{\rskarrow}{\ensuremath\xrightarrow{\RSK}}

\setcounter{MaxMatrixCols}{20}

\title{Symmetric Functions}
\author{Jasper Ty}
\date{}

\titleauthorhead

\begin{document}

\maketitle

These are notes based on my self-study of Chapter 7 in R.P Stanley's ``Enumerative Combinatorics'', mixed in with readings of various other expositions. 

\hide{I currently have a weak foundation in algebra, but am very comfortable with combinatorial arguments, so you might find that the algebraic aspects are over-explained while the combinatorial aspects are under-explained.}

\tableofcontents

\section*{Notation and conventions}
\addcontentsline{toc}{section}{Notation and conventions}

\subsection*{Sets}

We take $\NN$ to be the set of natural numbers \textit{including} zero,
\[
    \NN := \{0,1,2,\ldots\}.
\]
We take $\PP$ to be the set of \textit{positive integers},
\[
    \PP := \{1,2,\ldots\}.
\]
$\ZZ,\QQ,\RR,\CC$ are defined as usual.

\subsection*{Partitions and compositions}

A \textit{weak composition} $\alpha$ of $n \in \NN$ is an infinite tuple of nonnegative integers 
\[
    (\alpha_1, \alpha_2, \ldots)
\]
such that $\sum_i \alpha_i = n$. 
We define $|\alpha| = \sum_i \alpha_i$ to have notation for recovering $n$ given $\alpha$.

When more convenient (most of the time), we will omit brackets and commas, and write compositions as strings of digits. For example we will write $(1,1,4,3)$ as $1143$.

A \textit{partition} $\lambda$ of $n$ is a weak composition whose entries are \textit{weakly decreasing}. 
That a particular partition $\lambda$ is a partition of a particular $n$ is denoted $\lambda \vdash n$. 

I use English notation when drawing diagrams and tableaux, meaning, rows are drawn further downward as the index increases.

\subsection*{Rings, polynomials, and formal power series}

All rings considered are commutative and unital. An arbitrary ring will be denoted $\KK$. 

$\KK[[t]]$ will denote the formal power series ring over $\KK$ in the indeterminate $t$.

We will fix notation for the following sets of indeterminates:
\begin{enumerate}[label=(\alph*)]
    \item $X_N := (x_1, x_2, \ldots x_N)$ for a set of $N$ indeterminates.
    \item $X := (x_1, x_2, \ldots)$ for a set of countably many indeterminates.
    \item $Y$, $Y_N$, $Z$, $Z_N$, $Q$, $Q_N$ and so on are defined similarly.
\end{enumerate}

Let $\KK[[X]]$ be a formal power series ring. With compositions, partitions, or otherwise any finitely supported tuple of nonnegative integers $\alpha$, we define \textit{multi-index notation} for compactly writing down monomials in $\KK[[X]]$.
\[
    x^\alpha := x_1^{\alpha_1}x_2^{\alpha_2}x_3^{\alpha_3}\cdots.
\]
In the context of multi-index notation, $\alpha$ will also be called $x$'s \textit{exponent vector}.
We will let $[x^\alpha]f$ denote the coefficient of $[x^\alpha]$ in the formal power series $f$.

\subsection*{Permutations and the symmetric group}

$\frkS_n$ will denote the symmetric group on $n$ letters. I chose this symbol mostly because I think it looks insanely cool.

I use cycle notation, so e.g the cycle that sends $1$ to $7$, $7$ to $4$, and $4$ to $1$ will be written as $(174)$.

The simple transpositions $(i\:i+1)$ will be denoted $s_i$.

Permutations will act on functions by permuting places, so if $w \in \frkS_n$ and $f$ is a function, then

\[
    wf(x_1,\ldots,x_n) = f(x_{w(1)},\ldots,x_{w(n)}).
\]

\section{Symmetric Functions}

The first definition is exactly how the ring of symmetric functions is defined in \\
\cite{StanleyEC2}, which is was my primary reference on this topic.

\subsection{As a certain subring of FPS in countably many indeterminates}

\subsubsection{Definition}

\begin{definition}
    A \textit{A homogeneous symmetric function of degree $n$} over a ring $\KK$ is a formal power series
    \[
        \sum_\alpha c_\alpha x^\alpha \in \KK[[X]],
    \]
    where we are summing over all weak compositions $\alpha$ of $n$, and every $c_\alpha$ is a scalar.
    
    We denote the set of all such formal power series by $\Lambda_\KK^n$.
\end{definition}

These form a $\KK$-module, as a submodule of $\KK[[X]]$. 
The following is a simple example:

\begin{example}
    The formal power series 
    \[
        f = \sum_i x_i^2 + 10\sum_{i < j} x_ix_j
    \]
    is a symmetric function that is homogeneous of degree $2$. In this case $c_\alpha = 1$ whenever $\alpha = \ldots 2 \ldots$, and $c_\alpha = 10$ whenever $\alpha = \ldots 1 \ldots 1 \ldots$. 
\end{example}


We note that multiplying any two homogeneous symmetric functions $f$, $g$ of degree $m$ and $n$ respectively give us a homogeneous symmetric function of degree $m+n$.
The following definition gives us the space in which this multiplication makes sense.

\subsubsection{The ring of symmetric functions}

\begin{definition}
    The \textit{ring of symmetric functions} $\Lambda_\KK$ is the infinite direct sum
    \[
        \Lambda_\KK := \Lambda_\KK^0 \oplus\Lambda_\KK^1 \oplus \cdots.
    \]
    In the case when $\KK = \QQ$, we will suppress $\QQ$ and refer to $\Lambda_\QQ^n$ and $\Lambda_\QQ$ as $\Lambda^n$ and $\Lambda$ respectively.
\end{definition}

This direct sum is \textit{internal}, happening within $\KK[[X]]$. 
This is a $\KK$-algebra, being a subalgebra of $\KK[[X]]$.

This greatly broadens the possible definitions for symmetric functions. 
For example, the following demonstrates a symmetric function that arises from an infinite product, which evidently contains many monomials of different degrees and is not at all homogeneous.

\begin{example}
    The formal power series 
    \[
        f = \prod_i (1 + 5x_i^2 + 7x_i^5)
    \]
    is a symmetric function.
\end{example}

And in fact, we will find many such symmetric functions, usually in the form of \textit{Cauchy identities}.


\subsection{As a limit of graded rings}

The \textit{definition} of a symmetric function arises from the study of \textit{symmetric polynomials}. Specifically, it is a limit in the category of graded rings.

Consider the polynomial ring $K = \KK[x_1,\ldots,x_N]$. We denote the subring of $K$ which consists of all polynomials $f(x_1,\ldots,x_N)$ such that $wf = f$ for all $w \in \frkS_N$ to be the \textit{ring of symmetric polynomials in $N$ variables}.
We denote this subring by $\KK[x_1,\ldots,x_N]^{\frkS_N}$.


\section{Partitions}
\subsection{Definition}
\begin{definition}
    A partition $\lambda$ of $n \in \NN$, which we denote $\lambda \vdash n$, is a sequence of numbers 
    \[
        (\lambda_1,\lambda_2,\ldots)
    \]
    such that 
    \[
        \sum_{i\in\NN}\lambda_i = n
    \]
    and 
    \[
        \lambda_j \leq \lambda_k
    \]
    for all $j\geq k$.
\end{definition}

In other words, $\lambda_k$ is weakly decreasing and has only finitely many nonzero entries. 

The set of all partitions $\lambda$ such that $\lambda \vdash n$ is denoted $\Par(n)$. We define the set of \textit{all} partitions to be just $\Par$.

\subsection{Young diagrams}
Partitions can be drawn as \textit{Ferrers diagrams} and \textit{Young diagrams}. 

Both have the same underlying data structure: they encode the partition $\lambda$ as a subset of $\NN^2$.

\subsection{Orders}

\begin{definition}[Containment order]
    Young diagrams, as subsets of $\NN^2$, have a partial order induced by containment. 
    \textit{Containment order} for partitions is precisely this order when you pull back Young diagrams to partitions.

    We will use $\subseteq$ to denote this order.
\end{definition}

\begin{definition}[Dominance order]
    Let $\lambda$ and $\nu$ be two partitions. We say that $\lambda$ \textit{dominates} or \textit{majorizes} $\nu$ if
    \[
        \sum_{k=1}^i \lambda_k \geq \sum_{k=1}^i \nu_k \qquad \forall i\in\NN.
    \]
    We denote this relation $\lambda \leq \nu$.
\end{definition}

\begin{theorem}[The covering relation for dominance order]
    $\lambda$ covers $\nu$ if
\end{theorem}

\begin{definition}[Lexicographic order]
    We define the \textit{lexicographic order} on $\Par$ to be

    We denote this relation $\lambda \prec \nu$.
\end{definition}

\begin{theorem}[Lexicographic order is a total order]
\end{theorem}

\begin{theorem}[Dominance order embeds into lexicographic order]
\end{theorem}

\section{Distinguished bases of symmetric functions}

We will cover several interesting bases of the ring of symmetric functions. These are \textit{all} indexed by partitions.

\subsection{Monomial symmetric functions}

The first basis, the monomials, has the amazing property where it's \textit{remarkably obvious the fact that it even is a basis}.

This is in total analogy to taking the \textit{monomial basis of a polynomial ring}. In this case, we group together monomials by the orbits of the place-permuting $\frkS_n$ action.

\begin{definition}
    Let $\lambda \vdash n$. The \textit{monomial symmetric function} $m_\lambda$ is 
    \[
        m_\lambda := \sum_{\alpha \sim \lambda} x^\alpha.
    \]
    where $\alpha \sim \lambda$ means that $\alpha$ may be obtained by permuting the parts of $\lambda$.
\end{definition}

The above definition involves permuting around the exponent vector $\alpha$. I think it's actually much easier to think of the monomial symmetric functions as \textit{permuting the subscripts}. 

\begin{example}
    Let $\lambda = 5322$. Then
    \begin{align*}
        m_\lambda = \sum_{i_1<i_2<i_3<i_4} \Big(&x_{i_1}^5x_{i_2}^3x_{i_3}^2x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^3x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^2x_{i_4}^3 \\ 
                                           &+ x_{i_1}^2x_{i_2}^5x_{i_3}^3x_{i_4}^2 + x_{i_1}^2x_{i_2}^5x_{i_3}^2x_{i_4}^3 + x_{i_1}^2x_{i_2}^2x_{i_3}^5x_{i_4}^3  \\
                                           &+ x_{i_1}^3x_{i_2}^5x_{i_3}^2x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^5x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^2x_{i_4}^5  \\
                                           &+ x_{i_1}^2x_{i_2}^3x_{i_3}^5x_{i_4}^2 + x_{i_1}^2x_{i_2}^3x_{i_3}^2x_{i_4}^5 + x_{i_1}^2x_{i_2}^2x_{i_3}^3x_{i_4}^5 \Big).
    \end{align*}
    \todo{Add extra expression for monomial example}
\end{example}

Now we prove the main result. 

\begin{theorem}
    The monomial symmetric funtions form a basis for $\Lambda$.
\end{theorem}

\begin{proof}
    It's impossible to form a nontrivial linear combination of monomial symmetric functions that sum to zero.
\end{proof}

\subsection{Elementary symmetric functions}

Our next basis will be the \textit{elementary symmetric functions}, which we will refer to as the \textit{elementaries} or the $e$'s.

\begin{definition}
    Let $n\in\NN$. The \textit{elementary symmetric function} $e_n$ is defined to be
    \[
        e_n := \sum_{i_1<i_2<\ldots<i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $e_\lambda$ is defined to be
    \[
        e_\lambda := e_{\lambda_1}e_{\lambda_2}\cdots.
    \]
\end{definition}

Now we examine the relationship between the elementaries and the monomials.

\begin{definition}
    Let $\lambda \vdash n$. We define $M_{\lambda\mu}$ to be the coefficient of $m_\mu$ in the expansion of $e_\lambda$ in the monomial basis. That is, the numbers such that
    \[
        e_\lambda = \sum_\mu M_{\lambda\mu} m_\mu.
    \]
    More generally, for any weak composition $\alpha$, let $M_{\lambda\alpha}$ be the coefficient of $x^\alpha$ in $e_\lambda$.
\end{definition}

\begin{theorem}
    The coefficient $M_{\lambda\mu}$ is counted by zero-one matrices whose row-sums are $\lambda$ and whose column sums are $\mu$.
\end{theorem}

\begin{proof}

    Consider what is going on when we compute the terms of $e_\lambda$,
    \[
        e_\lambda = e_{\lambda_1}e_{\lambda_2}\ldots.
    \]
    The right hand side is
    \[
        \left(\sum_{i_1<\ldots<i_{\lambda_1}} x_{i_1}\cdots x_{\lambda_1}\right)\left(\sum_{i_1<\ldots<i_{\lambda_2}} x_{i_1}\cdots x_{\lambda_2}\right)\cdots
    \]
    and naming a term in this product means you pick up monomials which are products of $\lambda_1$ distinct variables from $\lambda_1$, monomials of $\lambda_2$ distinct variables from $\lambda_2$, and so on. 
    
    These choices of distinct variables are in fact subsets of $(x_1, x_2, \ldots)$, and encoding these subsets with lists of $1$s and $0$s gives us the rows of our $0-1$ matrix, where the row $i$ corresponds to $e_{\lambda_i}$.     
    \[
        \begin{bmatrix}
            x_1 & x_2 & x_3 & \cdots \\
            x_1 & x_2 & x_3 & \cdots \\
            x_1 & x_2 & x_3 & \cdots \\
            \vdots & \vdots & \vdots & \ddots 
        \end{bmatrix}
    \]
    The fact that we picked up $\lambda_i$ variables in each row manifests as the $i$-th row sum being equal to $\lambda_i$.
The exponent of a given variable $x_j$ appearing in a monomial only depends on how many times we picked up an $x_j$ from each $e_{\lambda_i}$. 
This manifests as the $j$-th column sum being equal to $\mu_i$.


\end{proof}

\begin{theorem} \label{e2msymmetric}
    Let $\lambda,\mu \vdash n$, then
    \[
        M_{\lambda\mu} = M_{\mu\lambda}.
    \]
\end{theorem}

\begin{proof}
    Matrix transposition is a bijection between the sets the two numbers count.
\end{proof}

\subsubsection{The fundamental theorem of symmetric functions}
\begin{theorem}[Gale-Ryser] \label{galeryser} Let $M = [a_{ij}]$ be a zero-one matrix, whose row sums are given by the composition $\alpha$ and whose column sums are given by composition $\beta$. Then it must be that $\alpha \leq \beta^T$. Moreover, there is only \textit{one} zero-one matrix such that $\alpha = \beta^T$.
\end{theorem}

\begin{proof}
    We demonstrate this algorithmically.
    \todo{Finish proof of Gale-Ryser}
\end{proof}
\begin{theorem}[Fundamental theorem of symmetric functions] The $e$'s form a $\ZZ$-basis for the ring of symmetric functions.
\end{theorem}

\begin{proof}
    By Theorem \ref{galeryser}, the transition matrix is upper-triangular and has $1$'s on the diagonal, hence it is invertible. 
\end{proof}

\subsection{Complete homogeneous symmetric functions}

The \textit{complete homogeneous symmetric functions}, or the \textit{completes}, or the $h$'s, have a very similar definition as the elementaries, but with distinctness relaxed.

\begin{definition}
    Let $n\in\NN$. The \textit{complete homogeneous symmetric function} $h_n$ is defined to be
    \[
        h_n := \sum_{i_1\leq i_2\leq\ldots\leq i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $h_\lambda$ is defined to be
    \[
        h_\lambda := h_{\lambda_1}h_{\lambda_2}\cdots.
    \]
\end{definition}

\begin{theorem}
    $N_{\lambda\nu}$ is counted by $\NN$-matrices with row sums $\lambda$ and column sums $\nu$.
\end{theorem}

\begin{theorem}
    Let $\lambda, \nu \vdash n$, then
    \[
        N_{\lambda\nu} = N_{\nu\lambda}.
    \]
\end{theorem}

\begin{proof}
    The exact same as with Theorem \ref{e2msymmetric}.
\end{proof}

\subsection{Power sum symmetric functions}

\begin{definition}
    Let $n\in\NN$. The \textit{power sum symmetric function} $p_n$ is defined to be
    \[
        p_n := \sum_{i} x_i^n
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $p_\lambda$ is defined to be
    \[
        p_\lambda := p_{\lambda_1}p_{\lambda_2}\cdots.
    \]
\end{definition}

\section{Some theorems}

\subsection{The Newton-Girard formulas}

\begin{theorem}[Newton-Girard formulas]
    Let $n \in \PP$. Then
    \begin{align}
        \sum_{k=0}^n (-1)^k e_kh_{n-k} &= 0 \label{ng1} \\
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k &= ne_n  \label{ng2} \\
        \sum_{k=0}^n h_{n-k}p_k &= nh_n \label{ng3}
    \end{align}
\end{theorem}

\begin{proof}
    First, we ``upgrade'' to proving certain power series identities, from which the Newton-Girard formulas can be read off. Consider the power series
    \[
        H(t) := \sum_{n\in\NN}h_n t^n \in \Lambda[[t]]
    \]
    and 
    \[
        E(t) := \sum_{n\in\NN}e_n t^n \in \Lambda[[t]].
    \]
    Combintorially, we can see that 
    \[
        H(t) = \prod_{n\in\NN}\frac{1}{1-x_nt}
    \]
    and
    \[
        E(t) = \prod_{n\in\NN}(1+x_nt).
    \]
    So we have that
    \[
        H(t)E(-t) = \prod_{n\in\NN}\frac{1-x_nt}{1-x_nt} = 1.
    \]
    Then $[t^n] H(t)E(-t) = 0$ for all $n \geq 1$, giving us
    \[
        \sum_{k=0}^n (-1)^k e_k h_{n-k} = 0 \qquad \forall n\geq 1.
    \]
    This proves the first Newton-Girard formula \eqref{ng1}.

    For the next, we define a new power series
    \[
        P(t) := \sum_{n\geq1}p_nt^n \in \Lambda[[t]].
    \]
    Which has the following formula
    \[
        P(t) = t\sum_{n\in\NN}\frac{x_n}{1-x_nt}
    \]
    Doing the same thing,
    \begin{align*}
        E(-t)P(t) &= \left[\prod_{n\in\NN}(1-x_nt)\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                  &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}(1-x_nt)\right] \\
                  &= t\sum_{m\in\NN} \left[x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right] \\
                  &= -t\sum_{m\in\NN} \left[-x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right].
    \end{align*}
    The sum can be expressed as the derivative of an infinite product, and we can continue the simplification
    \begin{align*}
                  &= -t\frac{d}{dt}\left[\prod_{m\in\NN}(1-x_mt)\right] \\
                  &= -t\frac{d}{dt}E(-t) \\
                  &= -t\frac{d}{dt}\left[\sum_{n\in\NN}(-1)^n e_nt^n\right] \\
                  &= -t\left[\sum_{n\geq 1}n(-1)^n e_nt^{n-1}\right] \\
                  &= \sum_{n\geq 1}n(-1)^{n-1} e_nt^n.
    \end{align*}
    Then, the formula for $[t^n]E(-t)P(t)$ given $n \geq 1$ is
    \[
        \sum_{k=0}^n (-1)^k e_{n-k}p_k = n(-1)^{n-1}e_n \qquad \forall n \geq 1.
    \]
    And after moving the $-1$ factors,
    \[
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k = ne_n.
    \]
    This proves the second Newton-Girard formula, \eqref{ng2}.

    The proof of the third is very similar and actually even easier, since
    \begin{align*}
        \frac{d}{dt}H(t) &= \frac{d}{dt}\left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right] \\
                         &= \sum_{m\in\NN}\left[\frac{x_m}{(1-x_mt)^2}\prod_{\substack{n\in\NN \\ n \neq m}}\frac{1}{1-x_nt}\right] \\ 
                         &= \sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}\frac{1}{1-x_nt}\right]
    \end{align*}
    Then
    \begin{align*}
        H(t)P(t) &= \left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                 &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod{n\in\NN}\frac{1}{1-x_nt}\right] \\
                 &= t\frac{d}{dt}H(t) \\
                 &= t\frac{d}{dt}\left[\sum_{n\in\NN}h_nt^n\right] \\
                 &= t\sum_{n\geq 1}nh_nt^{n-1} \\
                 &= \sum_{n\geq 1}nh_nt^n,
    \end{align*}
    which proves the third Newton-Girard formula, \eqref{ng3}.
\end{proof}

\subsection{Vieta's formulas}

\subsection{Cauchy identities}

\begin{theorem}\label{CauchyIdentityForME}
    We have that
    \[
        \prod_{i,j \geq 1} (1+x_iy_j) = \sum_{\lambda\in\Par} m_\lambda(X) e_\lambda(Y).
    \]
\end{theorem}

\begin{theorem}\label{CauchyIdentityForMH}
    We have that
    \[
        \prod_{i,j \geq 1} (1-x_iy_j)^{-1} = \sum_{\lambda\in\Par} m_\lambda(X) h_\lambda(Y).
    \]
\end{theorem}

\begin{theorem}\label{CauchyIdentityForPP}
    We have that
    \[
        \prod_{i,j \geq 1} (1-x_iy_j)^{-1} = \sum_{\lambda\in\Par} z_\lambda^{-1}p_\lambda(X) p_\lambda(Y).
    \]
\end{theorem}

\section{\texorpdfstring{$\omega$}{w}-involution}


\begin{theorem}
    For all $n$,
    \[
        \omega(h_n) = e_n.
    \]
    Thus, $\omega$ is an involution.
\end{theorem}

\begin{proof}
    We will use the first Newton-Girard formula \eqref{ng1}.
\end{proof}

\section{The Hall inner product}

We define an inner product $\langle \cdot, \cdot \rangle$ on $\Lambda$ via the following rule:

\begin{definition} Let $\langle\cdot,\cdot\rangle$ be the scalar product defined by the relationship
    \[
        \langle m_\lambda, h_\nu \rangle = \delta_{\lambda\nu}.
    \]
    This scalar product is called the \textit{Hall inner product}. It is well defined since the $m$'s and $h$'s form a basis for $\Lambda$.
\end{definition}

\begin{theorem}
    $\langle\cdot,\cdot\rangle$ is symmetric.
\end{theorem}

\begin{proof}
    It suffices to prove that products of basis elements are symmetric for some bases of $\Lambda$. Consider the basis $\{h_\lambda\}$. We have that
    \[
        \langle h_\lambda, h\nu \rangle = \left\langle \sum_{\gamma} N_{\lambda\gamma} m_\gamma, h\nu \right\rangle = N_{\lambda\nu}.
    \]
    Then $\langle h_\lambda, h_\nu \rangle = N_{\lambda\nu} = N_{\nu\lambda} = \langle h_\lambda, h_\nu \rangle$.
\end{proof}

\begin{theorem}\label{thm:CauchyIdentityImpliesOrthonormal}
    Any two bases $\{u_\lambda\}$ and $\{v_\lambda\}$ of $\Lambda$ that have a Cauchy identity
    \begin{equation}\label{eq:CauchyIdentityArbitrary}
        \prod_{i,j \geq 1}(1 - x_iy_j)^{-1} = \sum_{\lambda\in\Par} u_\lambda(X) y_\lambda(Y)
    \end{equation}
    are orthonormal with respect to the Hall inner product.
\end{theorem}

\begin{proof}
    Let
    \[
        m_\lambda = \sum_\rho \zeta_{\lambda\rho} u_\rho, \qquad h_\mu = \sum_\nu \eta_{\mu\nu} v_\nu.
    \]
    Then
    \[
        \delta_{\lambda\mu} = \langle m_\lambda, h_\mu \rangle = \sum_{\rho,\nu}\zeta_{\lambda\rho} \eta_{\mu\nu} \langle u_\rho, u_\nu \rangle
    \]
    If $\langle u_\lambda, v_\mu \rangle = \delta_{\lambda\mu}$, the above becomes
    \[
        \delta_{\lambda\mu} =  \sum_{\rho,\nu}\zeta_{\lambda\rho} \eta_{\mu\nu} \delta_{\rho\nu}.
    \]
    Since $\sum_\nu \eta_{\mu\nu}\delta_{\rho\nu} = \eta_{\mu\rho}$, we find that
    \begin{equation}\label{eq:CauchyIdentityImpliesDualBasis:1}
        \delta_{\lambda\mu} =  \sum_\rho\zeta_{\lambda\rho} \eta_{\mu\rho}.
    \end{equation}
    The sequence of equalities given can be run backwards, so we have to just show (\ref{eq:CauchyIdentityImpliesDualBasis:1}) and the theorem is proven.
    By Theorem \ref{CauchyIdentityForMH}, we have that
    \[
        \prod_{i,j}(1-x_iy_j)^{-1} = \sum_\lambda m_\lambda(X) h_\lambda(Y).
    \]
    So 
    \[
        \prod_{i,j}(1-x_iy_j)^{-1} = \sum_\lambda \left(\sum_\rho \zeta_{\lambda\rho} u_\rho(X)\right)\left(\sum_\nu \eta_{\lambda\nu}v_\nu(Y)\right).
    \]
    Interchanging sums,
    \[
        \sum_\lambda \left(\sum_\rho \zeta_{\lambda\rho} u_\rho(X)\right)\left(\sum_\nu \eta_{\lambda\nu}v_\nu(Y)\right) = \sum_{\rho,\nu} \left(\sum_\lambda \zeta_{\lambda\rho} \eta_{\lambda\nu}\right) u_\rho(X) v_\nu(Y).
    \]
    By (\ref{eq:CauchyIdentityArbitrary})
    \[
        \sum_{\rho,\nu} \left(\sum_\lambda \zeta_{\lambda\rho} \eta_{\lambda\nu}\right) u_\rho(X) v_\nu(Y) = \sum_\mu u_\mu(X) v_\mu(Y).
    \]
    Since the $u_\lambda(X)v_\lambda(Y)$ are linearly independent as power series, we can compare coefficients, which gives us the desired equality.
\end{proof}

\section{Schur functions}
\subsection{Combinatorial definitions}
\subsubsection{Schur functions via semistandard Young tableaux}

\begin{definition}
    A semistandard Young tableau is a Young diagram filled in with entries which increase \textit{weakly} along rows but \textit{strongly} along columns. 
    The set of all semistandard Young tableau of shape $\lambda$ is denoted $\SSYT(\lambda)$.

    The \textit{type} of a semistandard Young tableau $T$, $\type(T)$ is its set of entries counted with multiplicities. Equivalently, it's the tuple $(a_1, a_2, \ldots)$ defined by
    \[
        a_i = (\#\text{of times }i\text{ appears in }T)
    \]
\end{definition}

\begin{example}
    The following Young tableau is semistandard, of shape $4311$ and type $22221$.
    \[
        \begin{ytableau}
            1 & 1 & 2 & 4 \\
            2 & 3 & 3  \\
            4 \\
            5 \\ 
        \end{ytableau}
    \]
\end{example}

\begin{definition}
    Let $T$ be a semistandard Young tableau.  Then define the \textit{weight} of $T$, $x^T$, to be the monomial
    \[
        x^T := x_1^{a_1}x_2^{a_2}\cdots,
    \]
    where $a_1$ is the number of occurrences of $1$ in $T$, $a_2$ is the number of occurrences of $2$ in $T$, and so on.
\end{definition}

\begin{definition}[Schur functions via tableaux]
    Let $\lambda \vdash n$. The \textit{Schur function} $s_\lambda$ is defined to be
    \[
        s_\lambda := \sum_{T \in SSYT(\lambda)} x^T.
    \]
\end{definition}



\begin{example}
    For the partition $22$, we compute $s_\lambda(x_1,x_2,x_3)$.
    We have the following fillings that are semistandard Young tableaux:
    \[
        \ytableausetup{boxsize=normal}
        \begin{ytableau}
            1 & 1 \\
            2 & 2
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 1 \\
            2 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 2 \\
            2 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 1 \\
            3 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 2 \\
            3 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            2 & 2 \\
            3 & 3
        \end{ytableau}\:.
    \]
    These have the weights
    \[
        x_1^2x_2^2, \qquad 
        x_1^2x_2x_3, \qquad 
        x_1x_2^2x_3, \qquad 
        x_1^2x_3^2, \qquad 
        x_1x_2x_3^2, \qquad 
        x_2^2x_3^2
    \]
    respectively. Hence we have computed that
    \[
        s_{22}(x_1,x_2,x_3) = x_1^2x_2^2 + x_1^2x_3^2 + x_2^2x_3^2 + x_1^2x_2x_3 + x_1x_2^2x_3 + x_1x_2x_3^2.
    \]
\end{example}

\subsubsection{Skew Schur functions}
\begin{definition}
    Let $\lambda, \nu$ be partitions such that $\nu \subseteq \lambda$. Then the pair $(\lambda, \nu)$ is referred to as a \textit{skew shape} and is denoted $\lambda \setminus \nu$.

    The \textit{skew diagram} of $\lambda \setminus \nu$ is the diagram obtained by taking $\lambda$'s Young diagram and removing all boxes that would be contained in $\nu$'s Young diagram.

    Finally, a \textit{skew tableau of shape $\lambda \setminus \nu$} or a \textit{tableau of skew shape $\lambda \setminus \nu$} is a filling of the skew diagram of $\lambda \setminus \nu$. 

    Such a tableau will still be called \textit{semistandard} if it weakly increases along rows and strongly increases along columns.
\end{definition}

\begin{example}
    Let $\lambda = \ydiagram{4,4,3,2}$ and $\nu = \ydiagram{3,2,2}$. Then $\lambda \setminus \nu$ is

    \ytableausetup{boxsize=normal}
    \[\ydiagram{3+1,2+2,2+1,2}.\]
\end{example}

\subsubsection{Kostka numbers}

\begin{definition}
    Let $\lambda \vdash n$ and let $\alpha$ be a weak composition of $n$. The \textit{Kostka number } $K_{\lambda\alpha}$ counts the number of SSYT with shape $\lambda$ and type $\alpha$.

    Let $\nu \subseteq \lambda$. We define the skew Kostka number $K_{\lambda\setminus\nu,\alpha}$ similarly.
\end{definition}

\begin{remark}
    We have that
    \[
        s_\lambda = \sum_\alpha K_{\lambda\alpha}x^\alpha.
    \]
\end{remark}

\begin{theorem} The skew Schurs, and therefore also the Schurs, are symmetric functions.
\end{theorem}

\begin{proof}
    We will prove that $s_{\lambda\setminus\nu}$ is invariant under the action of simple transpositions. 
    Consider the simple transposition $s_i$ which swaps $i$ and $i+1$. 
    Let $\alpha$ be some weak composition of $n$, and define
    \[
        \tilde{\alpha} = s_i\alpha = (\alpha_1, \ldots, \alpha_{i+1}, \alpha_i, \ldots).
    \]
    We wish to now show that 
    \[
        K_{\lambda\setminus\nu,\alpha} = K_{\lambda\setminus\nu,\tilde{\alpha}}.
    \]
    We will prove this by bijection.

    The key fact here is that \textit{if $i$ and $i+1$ appear in the same column, they must be vertically adjacent}. 
    This is due to the column-strictness of semistandard Young tableau.

    Let $T\in\SSYT(\lambda \setminus \nu)$. 
    Take all the columns of $T$ which contain $i$ or $i+1$ but not both. 
    These will form rows consisting of consecutive $i$'s followed by consecutive $i+1$'s. 
    In these rows, swap the number of consecutive $i$'s with the number of consecutive $i+1$'s. 
    This swap works because the $i$'s that become $i+1$'s will not break column strictness since there is no $i+1$ in the same column, similarly $i+1$'s that become $i$'s will not break column strictness since there is no $i$ in the same column.

    This correspondence is involutive and therefore bijective. This completes the proof.

    For example, if $T$ looked like

    \ytableausetup{boxsize=20pt}
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & i & i &  \\
            \none & \none & \none & \none & \none & \none & \none & i & i+1 & i+1\\
            \none & \none & \none & i & i+1 & i+1 & i+1 & i+1 & & \\
            \none & \none & \none & i+1 &   \\
            i & i & i+1
        \end{ytableau},
    \]
    then we ignore all other entries with both $i$ and $i+1$.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red)i & i+1 & i+1 & i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            i & i & i+1
        \end{ytableau}
    \]
    and consider the remaining ones. Then, we keep track of the rows of consecutive $i$ and $i+1$'s.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(yellow) i+1 & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(yellow) i+1 & *(yellow) i+1 & *(yellow) i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(green) i & *(yellow) i+1
        \end{ytableau}
    \]
    Then, we flip the number of consecutive $i$ and $i+1$'s for each row.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(green) i & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(green) i & *(green) i & *(green) i & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(yellow) i+1 & *(yellow) i+1
        \end{ytableau}
    \]
\end{proof}

As a consequence,
\begin{corollary}
    Let $\lambda\setminus\nu$ be a skew shape. Then
    \[
        s_{\lambda\setminus\nu} = \sum_{\gamma}K_{\lambda\setminus\nu,\gamma}m_\gamma.
    \]
\end{corollary}

Next, we will prove that the Schurs $s_\lambda$ form a $\ZZ$-basis for $\Lambda$.

\begin{theorem}
    Let $\lambda, \nu \vdash n$. Then $K_{\lambda\nu} \neq 0$ implies $\lambda \geq \nu$. Also, $K_{\lambda\lambda} = 1$.
\end{theorem}

\begin{proof}
    Let $K_{\lambda\nu} \neq 0 $. Then we have the existence of a SSYT $T$ of shape $\lambda$ and type $\nu$. 

    We will show that the entries $1,\ldots,k$ can only appear in the first $k$ rows of $T$.
    Since there can only be $\lambda_1 + \cdots + \lambda_k$ entries in the first $k$ rows of $T$, this automatically tells us that
    \[
        \nu_1 + \cdots + \nu_k \leq \lambda_1 + \cdots + \lambda_k.
    \]

    Suppose that $k$ appeared in the row $i > k$, and let $T_{ij} = k$. 
    Then 
    \[
        1 \leq T_{1j} < T_{2j} < \cdots < T_{kj} < \cdots < T_{ij} = k.
    \]
    But that implies
    \[
        k \leq T_{kj} < \cdots < T_{ij} = k.
    \]
    which gives us $k < k$, a contradiction.

    If $\nu = \lambda$, the only possible SSYT has the $i$th row filled with $i$.
\end{proof}

\begin{corollary}
    $\{ s_\lambda : \lambda \in \Par(n) \}$ forms a basis for $\Lambda^n$, and therefore $\{ s_\lambda : \lambda \in \Par \}$ form a basis for $\Lambda$, as the transition matrix $(K_{\lambda\nu})_{n \prec \lambda, \nu \prec 1^n}$ is lower triangular.
\end{corollary}


\subsection{The Jacobi-Trudi identity}

\subsubsection{Statement}

\begin{theorem} Let $\lambda$ be a partition. Then
    \[
        s_\lambda = \det(h_{\lambda_i+j-i})_{i,j=1}^{\len(\lambda)}.
    \]
\end{theorem}


\subsubsection{The Lindstr\"om-Gessel-Viennot lemma}

\begin{definition} A \textit{digraph} $D$ is a pair consisting of a vertex set $V(D)$ and an arc set $A(D)$ which consists of ordered pairs of vertices. We will suppress the $D$ and refer to the vertex and arc sets as $V$ and $A$.
\end{definition}

\begin{definition}
    A \textit{path} $p$ in a digraph $D$ is an ordered list of arcs $(a_1, \ldots, a_k)$ which are connected end-to-end.
\end{definition}

\begin{definition}
    A \textit{cycle} is a path which starts and ends at the same vertex.
\end{definition}

\begin{definition}
    Let $D$ be a digraph.
    \begin{itemize}
        \item We say that $D$ is acyclic if it contains no cycles.
        \item We say that $D$ is path-finite whenever there exist only finitely many paths from $u$ to $v$ for all $u,v \in V$.
        \item Let $\KK$ be a ring. We say that $D$ is \textit{weighted} when we have a function $w: A \to \KK$ that assigns a \textit{weight} to each arc of $D$.
    \end{itemize}
\end{definition}

\begin{theorem}[Lindstr\"om-Gessel-Viennot] 
    Let $D$ be an weighted, acyclic path-finite, digraph.

    Let $U, V$ be two sets of $n$ vertices in $D$. Define the \textit{weight} of a path $p$ to be 
    \[
        w(p) = \prod_{a \in p} a
    \]
    For any two vertices $u, v$ of $D$, define the quantity $\phi(u, v)$ to be
    \[
        \phi(u,v) = \sum_{p:u \rightarrow v} w(p),
    \]
    where $p: u \rightarrow v$ means that $p$ is a path from $u$ to $v$.

    Consider now the determinant 

    \[
        \det \begin{bmatrix}
            \phi(u_1, v_1) & \cdots & \phi(u_n, v_1) \\
             \vdots & \ddots & \vdots \\
            \phi(u_1, v_n) & \cdots & \phi(u_n, v_n)
        \end{bmatrix}
    \]

    \todo{Finish proof of LGV}
\end{theorem}

\subsubsection{Proof}

\begin{proof}[Proof of the Jacobi-Trudi identity]
    Fix $N \in \NN$. Consider the digraph $D$ whose vertex set is $\NN \times \{1,\ldots,N\}$. 

    We assign weights to the edges so that all vertical arcs are weighted $1$, and all horizontal arcs $(i, j) \to (i+1, j)$ are weighted $x_{j+1}$.

    For example, consider the following path
    \begin{center}
        \begin{tikzpicture}[
            scale=2,
            font=\Large
            ]
            \draw[
                step=1cm,
                line width = 0.05cm,
                ] (0,0) grid (3,3);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (0,0) -- node[above] {$x_1$} ++(1,0);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (1,0) -- ++(0,1);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (1,1) -- ++(0,1);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (1,2) -- node[above] {$x_3$} ++(1,0);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (2,2) -- node[above] {$x_3$} ++(1,0);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (3,2) --++ (0,0.75);
            \filldraw[
                fill=white,
                draw=black,
                line width=0.05cm,
                radius=0.25cm,
                ] (0,0) circle node {$u$};
            \filldraw[
                fill=white,
                draw=black,
                line width=0.05cm,
                radius=0.25cm
                ] (3,3) circle node {$v$};
        \end{tikzpicture}
    \end{center}

    This path's weight is $x_1x_3^2$.

    Paths $p: (i,1) \to (i+n, N)$ are in bijection with monomials in $\KK[X_N]$ of degree $n$.



    We lay out the parts of $\lambda$ in ascending order at $x=1$.

    \todo{Finish proof of Jacobi-Trudi}
\end{proof}

\section{The Robinson-Schensted-Knuth correspondence}

I'm going to define RSK functionally instead of imperatively. 

(After the fact, I think this was a waste of time and was more or less a cacophony of notation, but it was interesting regardless.)

\subsection{Row insertion}

The basic operation will be that of \textit{row insertion}.

\begin{definition}[Row insertion]
    Let $P = (P_{ij})$ be a SSYT of shape $\lambda$. And let $t \in \NN$. We will define the \textit{insertion path} $I(P\leftarrow t)$ now.
    We write that $t \geq P_m$ whenever $t \geq \max P_m$ or $P_m$ is empty and $t < P_m$ otherwise. 
    If $t < P_m$, define
    \begin{align*}
        (P_m \leftarrow t) := \begin{cases}
            \lambda_m + 1 & \text{if $t \geq \max P_m$ or $P_m$ is empty} \\
            \min\{n:P_{mn} > t\} & \text{otherwise}.
        \end{cases}.
    \end{align*}
    We note that $(P_m \leftarrow t) \leq \lambda_m + 1$ always.
    We will define a recursive function $\RowInsert$ as follows.
    \begin{align*}
        &\RowInsert(P,m,t) \\
        &:= \begin{cases}
            [(P_m \leftarrow t)] & t \geq P_m \\
            [(P_m \leftarrow t)] + \RowInsert(P, m+1, P_{m(P_m \leftarrow t)}) & t < P_m
        \end{cases}.
    \end{align*}
    Where $+$ denotes list concatenation.
    Now, define $I(P\leftarrow t) := \RowInsert(P,1,t).$
\end{definition}

Now we define the tableau that results from row insertion, $(P \leftarrow t)$.

\begin{definition}
    Let $P$ be a SSYT and let $t \in \NN$. Let $I(P \leftarrow t) = [j_1, j_2, \ldots, j_M]$.
    \[
        (P \leftarrow t)_{rs} := \begin{cases}
            t & \text{if } (r,s) = (1, j_1) \\
            P_{m-1,j_{m-1}} & \text{if }(r,s) = (m, j_m) \\
            P_{rs} & \text{otherwise}.
        \end{cases}.
    \]
\end{definition}

We state a property of insertion paths that will allow us to prove that row insertion gives us SSYT.

\begin{theorem}
    Let $P$ be a SSYT of shape $\lambda$, let $t \in \PP$, and let $I(P\leftarrow t) = [j_1,j_2,\ldots,j_M]$. Then $I(P\leftarrow t)$ is weakly decreasing.
\end{theorem}

\begin{proof}
    Let $1 \leq m < M$.
    Suppose that $\lambda_{m+1} < j_m$. Then 
    \[
        j_{m+1} = (P_m \leftarrow k) \leq \lambda_m+1 \leq j_m.
    \]
    Now suppose that $\lambda_{m+1} \geq j_m$. 
    Then $P_{m+1,j_m}$ exists, and it must be that $P_{mj_m} < P_{m+1,j_m}$ due to column strictness. Then
    \[
        j_{m+1} = (P_{m+1} \leftarrow P_{mj_m}) = \min \{n:P_{m+1,n}>P_{mj_m}\} \leq j_m.
    \]
    Either way $j_{m+1} \leq j_m$.
\end{proof}

\begin{corollary}\label{InsertionPreservesSSYT}
    If $P$ is a SSYT and $t \in \PP$, then $(P \leftarrow t)$ is a SSYT.
\end{corollary}

\begin{proof}
    By the definition of row insertion, the rows are weakly increasing. Consider inserting a bumped number. By the previous theorem, it can only be moved down or down left, which means that it will always be inserted below a smaller number. This continues for the whole insertion path.
\end{proof}

\subsection{Generalized two-line notation}

\begin{definition}
    Let $A = (a_{ij})$ be a $\NN$-matrix. Let $i, j \in \NN$, and write a \textit{two-line entry} to be
    \[
        \binom{i}{j}.
    \]
    We concatenate two-line entries as follows
    \[
        \binom{i}{j} + \binom{k}{l} := \binom{i\:k}{j\:l}.
    \]
    Now consider the following formal sum:
    \begin{align*}
        w_A &= \sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij}\binom{i}{j} \\
            &= \sum_{i=1}^\infty \left(a_{i1}\binom{i}{1} + a_{i2}\binom{i}{2} + \cdots\right) \\
            &= \left(a_{11}\binom{1}{1} + a_{12}\binom{1}{2} + \cdots\right) + \left(a_{21}\binom{2}{1} + a_{22}\binom{2}{2} + \cdots\right) + \cdots.
    \end{align*}
    If $A$ has finite support, $w_A$ is well-defined. 
    $w_A$ is the \textit{two-line array} or \textit{generalized permutation} corresponding to the matrix $A$. We note that in $w_A$ the individual two-line entries are sorted in lexicographic order.
\end{definition}

\begin{example}
    Let $A$ be the matrix
    \[
        \begin{bmatrix}
            1 & 0 & 3 & 2 \\
            0 & 1 & 0 & 0 \\
            2 & 4 & 1 & 0 \\
            0 & 0 & 1 & 0
        \end{bmatrix}.
    \]
    Then 
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 & 1 & 1 & 1 \
              & 2 \
              & 3 & 3 & 3 & 3 & 3 & 3 & 3\
              & 4 \\
            1 \
              & 2 & 2 & 2 \
              & 4 & 4 \
              & 2 \
              & 1 & 1 \
              & 2 & 2 & 2 & 2 \
              & 3 \
              & 3
        \end{pmatrix}
    \]
\end{example}

\subsection{The RSK algorithm}

\begin{definition}
    We define the \textit{RSK correspondence} to be a rule that assigns to $\NN$-matrices of finite support to pairs of SSYT $(P, Q)$, which we notate
    \[
        A \rskarrow (P, Q).
    \]
    To compute $(P, Q)$, let 
    \[
        w_A = \begin{pmatrix}
            i_1 & i_2 & \cdots & i_m \\
            j_1 & j_2 & \cdots & j_m
        \end{pmatrix},
    \]
    and begin with $(P_0, Q_0) = (\varnothing, \varnothing)$. Iteratively construct $(P_{t+1}, Q_{t+1})$ from $(P_t,Q_t)$ as follows
    \begin{enumerate}[label=(\alph*)]
        \item $P_{t+1} = P_t \leftarrow j_{t+1}$
        \item Add a new box to $Q_t$ where a new box to $P_t$ was added, and fill it with $i_{t+1}$.
    \end{enumerate}
    This process continues until $t=m$, and we take $(P, Q) = (P_m, Q_m)$.

    $P$ is referred to as the \textit{insertion tableau} and $Q$ is referred to as the \textit{recording tableau}.
\end{definition}



\begin{example}
    Let 
    \[
        A := \begin{bmatrix}
            1 & 0 & 2 \\
            0 & 2 & 0 \\
            1 & 1 & 0
        \end{bmatrix}.
    \]
    We have that
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 \
              & 2 & 2 \
              & 3 & 3 \\
            1 \
              & 3 & 3 \
              & 2 & 2 \
              & 1 & 2
        \end{pmatrix}.
    \]
    So, going through the insertion process,

    \ytableausetup{boxsize=10pt}
    \begin{center}
        \begin{tabular}{r | l | l}
            $t$ &  $P_t$ & $Q_t$ \\
            \hline 
            $1$ \
              & \begin{ytableau} 
                  \none
                \end{ytableau} \
              & \begin{ytableau} 
                  \none
              \end{ytableau} \\
            $2$ \
              & \begin{ytableau} 
                    *(green) 1
                \end{ytableau} \
              & \begin{ytableau} 
                  *(green) 1
              \end{ytableau} \\
            $3$ \
              & \begin{ytableau} 
                  1 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & *(green) 1
              \end{ytableau} \\
            $4$ \
              & \begin{ytableau} 
                  1 & 3 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & *(green) 1
              \end{ytableau} \\
            $5$ \
              & \begin{ytableau} 
                  1 & *(yellow) 2 & 3 \\
                  *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1 \\
                  *(green) 2
              \end{ytableau} \\
            $6$ \
              & \begin{ytableau} 
                  1 & 2 & *(yellow) 2 \\
                  3 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1  \\
                  2 & *(green) 2
              \end{ytableau} \\
            $7$ \
              & \begin{ytableau} 
                  1 & *(yellow) 1 & 2 \\
                  *(yellow) 2 & 3 \\
                  *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1  \\
                  2 & 2 \\
                  *(green) 3
              \end{ytableau} \\
            $8$ \
              & \begin{ytableau} 
                  1 & 1 & 2 & *(green) 2 \\
                  2 & 3 \\
                  3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1 & *(green) 3 \\
                  2 & 2 \\
                  3
              \end{ytableau} \\
        \end{tabular} 
    \end{center}  

    We find that
    \[
        P = \ytableaushort{1122,23,3} \qquad Q = \ytableaushort{1113,22,3}.
    \]
\end{example}

\begin{theorem}
    RSK is a bijection between $\NN$-matrices $A$ of finite support and pairs of SSYT $(P,Q)$ such that
    \begin{align*}
        \type(P) &= \col(A), \\
        \type(Q) &= \row(A).
    \end{align*}
\end{theorem}

\begin{proof}
    That the types of $P$ and $Q$ correspond to row and column sums of $A$ is obvious-- $j$ coordinates with multiplicities get inserted into $P$, while $i$ coordinates with multiplicities get inserted into $Q$.
    That $P$ is a SSYT follows from (\ref{InsertionPreservesSSYT}).
    That $Q$ is a SSYT follows from properties of the insertion path.

    Now, it remains to prove that the RSK correspondence is a bijection. 
    RSK can actually be run backwards. 
    First, we use this to prove injectivity, by showing that running RSK backwards is actually the inverse of RSK. 
    Second, we use this to prove surjectivity, by showing that backwards RSK works for arbitrary pairs of SSYT.

\end{proof}

\subsection{Implications}


\begin{theorem}[Cauchy identity]    We have
    \begin{equation}\label{eq:CauchyIdentityForSchurs}
        \prod_{i,j \geq 1}(1-x_iy_j)^{-1} = \sum_{\lambda \in \Par}s_\lambda(X) s_\lambda(Y).     
    \end{equation}
\end{theorem}

\begin{proof}
    We have that
    \begin{align*}
        [x^\alpha y^\beta]\left(\prod_{i,j\geq1}(1-x_iy_j)^{-1}\right) &= \left(\substack{\#\text{ of $\NN$-matrices $(a_ij)$ such that}\\\row(a) = \alpha\text{, and }\col(a) = \beta}\right) \\
        [x^\alpha y^\beta]\left(\sum_{\lambda\in\Par} s_\lambda(X) s_\lambda(Y) \right) &= \left(\substack{\#\text{ of pairs $(P,Q)$ of SSYT of the same shape such that}\\\type(P) = \alpha\text{, and }\type(Q) = \beta}\right).
    \end{align*}
    RSK tells us that both counts are the same, hence both sides of the identity are equal as power series.
\end{proof}

\begin{corollary}
    The Schurs are an orthonormal basis of $\Lambda$ with respect to the Hall inner product.
\end{corollary}
\begin{proof}
    This follows from Theorem \ref{thm:CauchyIdentityImpliesOrthonormal}.
\end{proof}

\begin{corollary}
    Let $\mu,\nu \vdash n$. Then
    \[
        \sum_{\lambda \vdash n} K_{\lambda\mu}K_{\lambda\nu} = \langle h_\mu, h_\nu \rangle.
    \]
\end{corollary}

\begin{proof}
    Take the coefficient of $x^\mu y^\nu$ in (\ref{eq:CauchyIdentityForSchurs}).
\end{proof}

\begin{corollary}\label{thm:HToSIsKostka}
    We have that
    \[
        h_\mu = \sum_\lambda K_{\lambda\mu}s_\lambda.
    \]
    Equivalently,
    \[
        \langle h_\mu, s_\lambda \rangle = K_{\lambda\mu}.
    \]
\end{corollary}

The book gives three proofs of this corollary. The first one is ``the fastest''.

\begin{proof}[Proof via the Hall inner product]
    We know that
    \[
        s_\lambda = \sum_\nu K_{\lambda\nu} m_\nu,
    \]
    so 
    \begin{align*}
        \langle h_\mu, s_\lambda \rangle &= \left\langle h_\mu, \sum_\nu K_{\lambda\nu} m_\nu \right\rangle \\
                                         &=\sum_{\nu} K_{\lambda\nu} \langle h_\mu, m_\nu \rangle \\
                                         &=\sum_{\nu} K_{\lambda\nu} \delta_{\mu\nu} \\
                                         &= K_{\lambda\mu}.
    \end{align*}
\end{proof}

This next one is really the same thing, but packaged differently.

\begin{proof}[Proof via Cauchy Identities]
    We have that
    \[
        \sum_\lambda m_\lambda(X)h_\lambda(Y) = \sum_\lambda s_\lambda(X) s_\lambda(Y),
    \]
    since both equal $\prod_{i,j}(1-x_iy_j)^{-1}$.
    So we have that
    \begin{align*}
        \sum_\mu m_\mu(X)h_\mu(Y) &= \sum_\lambda \left( \sum_\mu K_{\lambda\mu} m_\mu(X) \right) s_\lambda(Y) \\
                                  &= \sum_\lambda \sum_\mu m_\mu(X) K_{\lambda\mu} s_\lambda(Y) \\
                                  &= \sum_\mu \sum_\lambda m_\mu(X) K_{\lambda\mu} s_\lambda(Y) \\
                                  &= \sum_\mu m_\mu(X) \left(\sum_\lambda K_{\lambda\mu} s_\lambda(Y)\right)
    \end{align*}
    We already know that the $m_\mu(X)$'s are linearly independent; we finish the proof by equating their coefficients.
\end{proof}

This last proof is purely combinatorial. 

\begin{proof}[Proof via RSK]
    \[
        h_\mu = \sum_A x^{\col A}
    \]
\end{proof}

\begin{corollary}
    We have that
    \[
        h_{1^n} = \sum_{\lambda \vdash n} f^\lambda s_\lambda.
    \]
\end{corollary}

\begin{proof}
    Combine Corollary \ref{thm:HToSIsKostka} and the fact that $f^\lambda = K_{\lambda,1^n}$.
\end{proof}

\subsection{Standardization}

\subsection{Symmetry}
\begin{theorem}
    If
    \[
        A \rskarrow (P,Q),
    \]
    then
    \[
        A^T \rskarrow (Q,P).
    \]
\end{theorem}

\section{Appendix}

\begin{theorem}
    Googoo gaga
\end{theorem}

\section{Solutions to Exercises}

\begin{exercise}
\end{exercise}

\begin{thebibliography}{999999}
    \raggedright\footnotesize

    \bibitem[StanleyEC2]{StanleyEC2}
    Richard P. Stanley, \textit{Enumerative Combinatorics. Volume 2}, Cambridge University Press 2023.

    \bibitem[GrinbergAC]{DarijAC}
    Darij Grinberg, \textit{An Introduction to Algebraic Combinatorics}, \href{http://www.cip.ifi.lmu.de/~grinberg/t/21s/lecs.pdf}{http://www.cip.ifi.lmu.de/~grinberg/t/21s/lecs.pdf}

    \bibitem[WildonSymFuncs]{WildonSymFuncs}
    Mark Wildon, \textit{An Involutive Introduction to Symmetric Functions}, \href{http://www.ma.rhul.ac.uk/~uvah099/Maths/Sym/SymFuncs2020.pdf}{http://www.ma.rhul.ac.uk/~uvah099/Maths/Sym/SymFuncs2020.pdf}
\end{thebibliography}

\end{document}
