\documentclass{article}

\usepackage[garamond]{jaspercommon}
\usepackage{ebgaramond}
\usepackage{ytableau}

\newcommand{\frkS}{\ensuremath\mathfrak{S}}
\newcommand{\rskarrow}{\ensuremath\xrightarrow{\text{RSK}}}

\DeclareMathOperator{\RowInsert}{RowInsert}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\shape}{shape}
\DeclareMathOperator{\type}{type}
\DeclareMathOperator{\Par}{Par}
\DeclareMathOperator{\SSYT}{SSYT}
\DeclareMathOperator{\Dom}{Dom}


\setcounter{MaxMatrixCols}{20}

\title{Symmetric Functions}
\author{Jasper Ty}
\date{}

\titleauthorhead

\begin{document}

\maketitle

These are notes based on my self-study of Chapter 7 in R.P Stanley's ``Enumerative Combinatorics'', mixed in with readings of various other expositions. 

\hide{I currently have a weak foundation in algebra, but am very comfortable with combinatorial arguments, so you might find that the algebraic aspects are over-explained while the combinatorial aspects are under-explained.}

\tableofcontents

\section{Notation}
We take $\NN$ to be the set of natural numbers \textit{including} zero,
\[
    \NN := \{0,1,2,\ldots\}.
\]
We take $\PP$ to be the set of \textit{positive integers},
\[
    \PP := \{1,2,\ldots\}.
\]
$\ZZ,\QQ,\RR,\CC$ are defined as usual.

A \textit{weak composition} $\alpha$ of $n \in \NN$ is an infinite tuple of nonnegative integers 
\[
    (\alpha_1, \alpha_2, \ldots)
\]
such that $\sum_i \alpha_i = n$.

A \textit{partition} $\lambda$ of $n$ is a weak composition whose entries are \textit{weakly decreasing}. That a particular partition $\lambda$ is a partition of a particular $n$ is denoted $\lambda \vdash n$.

All rings considered are commutative and unital. An arbitrary ring will be denoted $\KK$.

$\KK[[t]]$ will denote the formal power series ring over $\KK$ in the indeterminate $t$.

Let $\KK[[x_1,x_2,\ldots]]$ be a formal power series ring. With compositions, partitions, or otherwise any finitely supported tuple of nonnegative integers $\alpha$, we define \textit{multi-index notation} $x^\alpha$ as the following monomial in $\KK[[x_1,x_2,\ldots]]$:
\[
    x^\alpha := x_1^{\alpha_1}x_2^{\alpha_2}x_3^{\alpha_3}\cdots.
\]

$\frkS_n$ denotes the symmetric group on $n$ letters.

\section{Symmetric Functions}
\subsection{As a subring of formal power series in countably many indeterminates}

\subsubsection{Homogeneous symmetric functions}

\begin{definition}[Homogeneous symmetric functions]
    A \textit{A homogeneous symmetric function of degree $n$} over a ring $\KK$ is a formal power series
    \[
        \sum_\alpha c_\alpha x^\alpha \in \KK[[x_1,x_2\ldots]],
    \]
    where we are summing over all weak compositions $\alpha$ of $n$, and every $c_\alpha$ is a scalar.
    
    We denote the set of all such functions by $\Lambda_\KK^n$.
\end{definition}

These form a $\KK$-module.

\subsubsection{The ring of symmetric functions}

\begin{definition}
    The \textit{ring} (algebra over $\QQ$) of symmetric functions $\Lambda_\QQ$ is the infinite direct sum
    \[
        \Lambda_\QQ := \Lambda_\QQ^0 \oplus\Lambda_\QQ^1 \oplus \cdots.
    \]
    We will suppress $\QQ$ and refer to $\Lambda_\QQ^n$ and $\Lambda_\QQ$ as $\Lambda^n$ and $\Lambda$ respectively.
\end{definition}


The \textit{definition} of a symmetric function arises from the study of \textit{symmetric polynomials}. Specifically, it is a limit of


\subsection{As a limit of graded rings}

Th

\section{Partitions}
\subsection{Definition}
\begin{definition}
    A partition $\lambda$ of $n \in \NN$, which we denote $\lambda \vdash n$, is a sequence of numbers 
    \[
        (\lambda_1,\lambda_2,\ldots)
    \]
    such that 
    \[
        \sum_{i\in\NN}\lambda_i = n
    \]
    and 
    \[
        \lambda_j \leq \lambda_k
    \]
    for all $j\geq k$.
\end{definition}

In other words, $\lambda_k$ is weakly decreasing and has only finitely many nonzero entries. 

The set of all partitions $\lambda$ such that $\lambda \vdash n$ is denoted $\Par(n)$. We define the set of \textit{all} partitions to be just $\Par$.

\subsection{Young diagrams}
Partitions are often drawn as either 

\subsection{Orders}

\begin{definition}[Containment order]
    Young diagrams, as subsets of $\NN^2$, have a partial order induced by containment. 
    \textit{Containment order} for partitions is precisely this order when you pull back Young diagrams to partitions.

    We will use $\subseteq$ to denote this order.
\end{definition}

\begin{definition}[Dominance order]
    Let $\lambda$ and $\nu$ be two partitions. We say that $\lambda$ \textit{dominates} or \textit{majorizes} $\nu$ if
    \[
        \sum_{k=1}^i \lambda_k \geq \sum_{k=1}^i \nu_k \qquad \forall i\in\NN.
    \]
    We denote this relation $\lambda \leq \nu$.
\end{definition}

\begin{theorem}[The covering relation for dominance order]
    $\lambda$ covers $\nu$ if
\end{theorem}

\begin{definition}[Lexicographic order]
    We define the \textit{lexicographic order} on $\Par$ to be
\end{definition}

\begin{theorem}[Lexicographic order is a total order]
\end{theorem}

\begin{theorem}[Dominance order embeds into lexicographic order]
\end{theorem}

\section{Distinguished bases of symmetric functions}

We will cover bases of the ring of symmetric functions. These are all indexed by partitions.

\subsection{Monomial symmetric functions}

\begin{definition}
    Let $\lambda \vdash n$. The \textit{monomial symmetric function} $m_\lambda$ is 
    \[
        m_\lambda := \sum_{\alpha \sim \lambda} x^\alpha.
    \]
    where $\alpha \sim \lambda$ means that $\alpha$ may be obtained by permuting the parts of $\lambda$.
\end{definition}

\begin{example}
    Let $\lambda = 5322$. Then
    \begin{align*}
        m_\lambda = \sum_{i_1<i_2<i_3<i_4} \Big(&x_{i_1}^5x_{i_2}^3x_{i_3}^2x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^3x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^2x_{i_4}^3 \\ 
                                           &+ x_{i_1}^2x_{i_2}^5x_{i_3}^3x_{i_4}^2 + x_{i_1}^2x_{i_2}^5x_{i_3}^2x_{i_4}^3 + x_{i_1}^2x_{i_2}^2x_{i_3}^5x_{i_4}^3  \\
                                           &+ x_{i_1}^3x_{i_2}^5x_{i_3}^2x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^5x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^2x_{i_4}^5  \\
                                           &+ x_{i_1}^2x_{i_2}^3x_{i_3}^5x_{i_4}^2 + x_{i_1}^2x_{i_2}^3x_{i_3}^2x_{i_4}^5 + x_{i_1}^2x_{i_2}^2x_{i_3}^3x_{i_4}^5 \Big).
    \end{align*}
\end{example}

\begin{theorem}
    The monomial symmetric funtions form a basis for $\Lambda$.
\end{theorem}

\begin{proof}
    It's impossible to form a nontrivial linear combination of monomial symmetric functions that sum to zero.
\end{proof}

\subsection{Elementary symmetric functions}

\begin{definition}
    Let $n\in\NN$. The \textit{elementary symmetric function} $e_n$ is defined to be
    \[
        e_n := \sum_{i_1<i_2<\ldots<i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $e_\lambda$ is defined to be
    \[
        e_\lambda := e_{\lambda_1}e_{\lambda_2}\cdots.
    \]
\end{definition}


\begin{theorem}
    The coefficient $M_{\lambda\nu}$ is counted by zero-one matrices whose row-sums are $\lambda$ and whose column sums are $\nu$.
\end{theorem}

\begin{theorem} \label{e2msymmetric}
    Let $\lambda,\nu \vdash n$, then
    \[
        M_{\lambda\nu} = M_{\nu\lambda}.
    \]
\end{theorem}

\begin{proof}
    Matrix transposition is a bijection between the sets the two numbers count.
\end{proof}

\subsubsection{The fundamental theorem of symmetric functions}
\begin{theorem}[Gale-Ryser] \label{galeryser} Let $M = [a_{ij}]$ be a zero-one matrix, whose row sums are given by the composition $\alpha$ and whose column sums are given by composition $\beta$. Then it must be that $\alpha \leq \beta^T$. Moreover, there is only \textit{one} zero-one matrix such that $\alpha = \beta^T$.
\end{theorem}

\begin{proof}
    We demonstrate this algorithmically.
\end{proof}
\begin{theorem}[Fundamental theorem of symmetric functions] The $e$'s form a $\ZZ$-basis for the ring of symmetric functions.
\end{theorem}

\begin{proof}
    By Theorem \ref{galeryser}, the transition matrix is upper-triangular and has $1$'s on the diagonal, hence it is invertible. 
\end{proof}

\subsection{Complete homogeneous symmetric functions}

\begin{definition}
    Let $n\in\NN$. The \textit{complete homogeneous symmetric function} $h_n$ is defined to be
    \[
        h_n := \sum_{i_1\leq i_2\leq\ldots\leq i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $h_\lambda$ is defined to be
    \[
        h_\lambda := h_{\lambda_1}h_{\lambda_2}\cdots.
    \]
\end{definition}

\begin{theorem}
    $N_{\lambda\nu}$ is counted by $\NN$-matrices with row sums $\lambda$ and column sums $\nu$.
\end{theorem}

\begin{theorem}
    Let $\lambda, \nu \vdash n$, then
    \[
        N_{\lambda\nu} = N_{\nu\lambda}.
    \]
\end{theorem}

\begin{proof}
    The exact same as with Theorem \ref{e2msymmetric}.
\end{proof}
    
\subsection{Power sum symmetric functions}

\begin{definition}
    Let $n\in\NN$. The \textit{power sum symmetric function} $p_n$ is defined to be
    \[
        p_n := \sum_{i} x_i^n
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $p_\lambda$ is defined to be
    \[
        p_\lambda := p_{\lambda_1}p_{\lambda_2}\cdots.
    \]
\end{definition}

\section{Some theorems}

\subsection{The Newton-Girard formulas}

\begin{theorem}[Newton-Girard formulas]
    Let $n \in \PP$. Then
    \begin{align}
        \sum_{k=0}^n (-1)^k e_kh_{n-k} &= 0 \label{ng1} \\
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k &= ne_n  \label{ng2} \\
        \sum_{k=0}^n h_{n-k}p_k &= nh_n \label{ng3}
    \end{align}
\end{theorem}

\begin{proof}
    First, we ``upgrade'' to proving certain power series identities, from which the Newton-Girard formulas can be read off. Consider the power series
    \[
        H(t) := \sum_{n\in\NN}h_n t^n \in \Lambda[[t]]
    \]
    and 
    \[
        E(t) := \sum_{n\in\NN}e_n t^n \in \Lambda[[t]].
    \]
    Combintorially, we can see that 
    \[
        H(t) = \prod_{n\in\NN}\frac{1}{1-x_nt}
    \]
    and
    \[
        E(t) = \prod_{n\in\NN}(1+x_nt).
    \]
    So we have that
    \[
        H(t)E(-t) = \prod_{n\in\NN}\frac{1-x_nt}{1-x_nt} = 1.
    \]
    Then $[t^n] H(t)E(-t) = 0$ for all $n \geq 1$, giving us
    \[
        \sum_{k=0}^n (-1)^k e_k h_{n-k} = 0 \qquad \forall n\geq 1.
    \]
    This proves the first Newton-Girard formula \eqref{ng1}.

    For the next, we define a new power series
    \[
        P(t) := \sum_{n\geq1}p_nt^n \in \Lambda[[t]].
    \]
    Which has the following formula
    \[
        P(t) = t\sum_{n\in\NN}\frac{x_n}{1-x_nt}
    \]
    Doing the same thing,
    \begin{align*}
        E(-t)P(t) &= \left[\prod_{n\in\NN}(1-x_nt)\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                  &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}(1-x_nt)\right] \\
                  &= t\sum_{m\in\NN} \left[x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right] \\
                  &= -t\sum_{m\in\NN} \left[-x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right].
    \end{align*}
    The sum can be expressed as the derivative of an infinite product, and we can continue the simplification
    \begin{align*}
                  &= -t\frac{d}{dt}\left[\prod_{m\in\NN}(1-x_mt)\right] \\
                  &= -t\frac{d}{dt}E(-t) \\
                  &= -t\frac{d}{dt}\left[\sum_{n\in\NN}(-1)^n e_nt^n\right] \\
                  &= -t\left[\sum_{n\geq 1}n(-1)^n e_nt^{n-1}\right] \\
                  &= \sum_{n\geq 1}n(-1)^{n-1} e_nt^n.
    \end{align*}
    Then, the formula for $[t^n]E(-t)P(t)$ given $n \geq 1$ is
    \[
        \sum_{k=0}^n (-1)^k e_{n-k}p_k = n(-1)^{n-1}e_n \qquad \forall n \geq 1.
    \]
    And after moving the $-1$ factors,
    \[
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k = ne_n.
    \]
    This proves the second Newton-Girard formula, \eqref{ng2}.

    The proof of the third is very similar and actually even easier, since
    \begin{align*}
        \frac{d}{dt}H(t) &= \frac{d}{dt}\left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right] \\
                         &= \sum_{m\in\NN}\left[\frac{x_m}{(1-x_mt)^2}\prod_{\substack{n\in\NN \\ n \neq m}}\frac{1}{1-x_nt}\right] \\ 
                         &= \sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}\frac{1}{1-x_nt}\right]
    \end{align*}
    Then
    \begin{align*}
        H(t)P(t) &= \left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                 &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod{n\in\NN}\frac{1}{1-x_nt}\right] \\
                 &= t\frac{d}{dt}H(t) \\
                 &= t\frac{d}{dt}\left[\sum_{n\in\NN}h_nt^n\right] \\
                 &= t\sum_{n\geq 1}nh_nt^{n-1} \\
                 &= \sum_{n\geq 1}nh_nt^n,
    \end{align*}
    which proves the third Newton-Girard formula, \eqref{ng3}.
\end{proof}

\subsection{Vieta's formulas}

\subsection{Power series identities}

\section{\texorpdfstring{$\omega$}{w}-involution}


\begin{theorem}
    For all $n$,
    \[
        \omega(h_n) = e_n.
    \]
    Thus, $\omega$ is an involution.
\end{theorem}

\begin{proof}
    We will use the first Newton-Girard formula \eqref{ng1}.
\end{proof}

\section{The Hall inner product}

We define an inner product $\langle \cdot, \cdot \rangle$ on $\Lambda$ via the following rule:

\begin{definition} Let $\langle\cdot,\cdot\rangle$ be the scalar product defined by the relationship
    \[
        \langle m_\lambda, h_\nu \rangle = \delta_{\lambda\nu}.
    \]
    This scalar product is called the \textit{Hall inner product}. It is well defined since the $m$'s and $h$'s form a basis for $\Lambda$.
\end{definition}

\section{Schur functions}
\subsection{Combinatorial definitions}
\subsubsection{Schur functions via semistandard Young tableaux}

\begin{definition}
    A semistandard Young tableau is a Young diagram filled in with entries which increase \textit{weakly} along rows but \textit{strongly} along columns. 
    The set of all semistandard Young tableau of shape $\lambda$ is denoted $\SSYT(\lambda)$.

    The \textit{type} of a semistandard Young tableau $T$, $\type(T)$ is its set of entries counted with multiplicities. Equivalently, it's the tuple $(a_1, a_2, \ldots)$ defined by
    \[
        a_i = (\#\text{of times }i\text{ appears in }T)
    \]
\end{definition}

\begin{example}
    The following Young tableau is semistandard, of shape $4311$ and type $22221$.
    \[
        \begin{ytableau}
            1 & 1 & 2 & 4 \\
            2 & 3 & 3  \\
            4 \\
            5 \\ 
        \end{ytableau}
    \]
\end{example}

\begin{definition}
    Let $T$ be a semistandard Young tableau.  Then define the \textit{weight} of $T$, $x^T$, to be the monomial
    \[
        x^T := x_1^{a_1}x_2^{a_2}\cdots,
    \]
    where $a_1$ is the number of occurrences of $1$ in $T$, $a_2$ is the number of occurrences of $2$ in $T$, and so on.
\end{definition}

\begin{definition}[Schur functions via tableaux]
    Let $\lambda \vdash n$. The \textit{Schur function} $s_\lambda$ is defined to be
    \[
        s_\lambda := \sum_{T \in SSYT(\lambda)} x^T.
    \]
\end{definition}



\begin{example}
    For the partition $22$, we compute $s_\lambda(x_1,x_2,x_3)$.
    We have the following fillings that are semistandard Young tableaux:
    \[
        \ytableausetup{boxsize=normal}
        \begin{ytableau}
            1 & 1 \\
            2 & 2
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 1 \\
            2 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 2 \\
            2 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 1 \\
            3 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 2 \\
            3 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            2 & 2 \\
            3 & 3
        \end{ytableau}\:.
    \]
    These have the weights
    \[
        x_1^2x_2^2, \qquad 
        x_1^2x_2x_3, \qquad 
        x_1x_2^2x_3, \qquad 
        x_1^2x_3^2, \qquad 
        x_1x_2x_3^2, \qquad 
        x_2^2x_3^2
    \]
    respectively. Hence we have computed that
    \[
        s_{22}(x_1,x_2,x_3) = x_1^2x_2^2 + x_1^2x_3^2 + x_2^2x_3^2 + x_1^2x_2x_3 + x_1x_2^2x_3 + x_1x_2x_3^2.
    \]
\end{example}

\subsubsection{Skew Schur functions}
\begin{definition}
    Let $\lambda, \nu$ be partitions such that $\nu \subseteq \lambda$. Then the pair $(\lambda, \nu)$ is referred to as a \textit{skew shape} and is denoted $\lambda \setminus \nu$.

    The \textit{skew diagram} of $\lambda \setminus \nu$ is the diagram obtained by taking $\lambda$'s Young diagram and removing all boxes that would be contained in $\nu$'s Young diagram.

    Finally, a \textit{skew tableau of shape $\lambda \setminus \nu$} or a \textit{tableau of skew shape $\lambda \setminus \nu$} is a filling of the skew diagram of $\lambda \setminus \nu$. 

    Such a tableau will still be called \textit{semistandard} if it weakly increases along rows and strongly increases along columns.
\end{definition}

\begin{example}
    Let $\lambda = \ydiagram{4,4,3,2}$ and $\nu = \ydiagram{3,2,2}$. Then $\lambda \setminus \nu$ is
    \ytableausetup{boxsize=normal}
    \[\ydiagram{3+1,2+2,2+1,2}.\]
\end{example}

\begin{theorem} The skew Schurs, and therefore also the Schurs, are symmetric functions.
\end{theorem}

\begin{proof}
    We will prove that $s_{\lambda\setminus\nu}$ is invariant under the action of simple transpositions. Consider the simple transposition which swaps $i$ and $i+1$. 

    The key fact is that \textit{if $i$ and $i+1$ appear in the same column, they must be vertically adjacent}.

    Let $T\in\SSYT(\lambda \setminus \nu)$. 

    Take all the columns of $T$ which contain $i$ or $i+1$ but not both. 
    These will form rows consisting of consecutive $i$'s followed by consecutive $i+1$'s. 
    In these rows, swap the number of consecutive $i$'s with the number of consecutive $i+1$'s. 
    This swap works because the $i$'s that become $i+1$'s will not break column strictness since there is no $i+1$ in the same column, similarly $i+1$'s that become $i$'s will not break column strictness since there is no $i$ in the same column.

    This correspondence is involutive and therefore bijective. 
    We've bijected SSYT of a given type to another whose type is a permutation of the original type.
    This tells us that $s$ is symmetric.

    For example, if $T$ looked like

    \ytableausetup{boxsize=20pt}
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & i & i &  \\
            \none & \none & \none & \none & \none & \none & \none & i & i+1 & i+1\\
            \none & \none & \none & i & i+1 & i+1 & i+1 & i+1 & & \\
            \none & \none & \none & i+1 &   \\
            i & i & i+1
        \end{ytableau},
    \]
    then we ignore all other entries with both $i$ and $i+1$.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red)i & i+1 & i+1 & i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            i & i & i+1
        \end{ytableau}
    \]
    and consider the remaining ones. Then, we keep track of the rows of consecutive $i$ and $i+1$'s.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(yellow) i+1 & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(yellow) i+1 & *(yellow) i+1 & *(yellow) i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(green) i & *(yellow) i+1
        \end{ytableau}
    \]
    Then, we flip the number of consecutive $i$ and $i+1$'s for each row.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(green) i & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(green) i & *(green) i & *(green) i & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(yellow) i+1 & *(yellow) i+1
        \end{ytableau}
    \]
\end{proof}

\subsection{Jacobi-Trudi identities}

\subsubsection{The Lindstr\"om-Gessel-Viennot lemma}

\begin{definition} A \textit{digraph} $D$ is a pair consisting of a vertex set $V(D)$ and an arc set $A(D)$ which consists of ordered pairs of vertices. We will suppress the $D$ and refer to the vertex and arc sets as $V$ and $A$.
\end{definition}

\begin{definition}
    A \textit{path} $p$ in a digraph $D$ is an ordered list of arcs $(a_1, \ldots, a_k)$ which are connected end-to-end.
\end{definition}

\begin{definition}
    A \textit{cycle} is a path which starts and ends at the same vertex.
\end{definition}

\begin{definition}
    Let $D$ be a digraph.
    \begin{itemize}
        \item We say that $D$ is acyclic if it contains no cycles.
        \item We say that $D$ is path-finite whenever there exist only finitely many paths from $u$ to $v$ for all $u,v \in V$.
        \item Let $\KK$ be a ring. We say that $D$ is \textit{weighted} when we have a function $w: A \to \KK$ that assigns a \textit{weight} to each arc of $D$.
    \end{itemize}
\end{definition}

\begin{theorem}[Lindstr\"om-Gessel-Viennot] 
    Let $D$ be an weighted, acyclic path-finite, digraph.

    Let $U, V$ be two sets of $n$ vertices in $D$. Define the \textit{weight} of a path $p$ to be 
    \[
        w(p) = \prod_{a \in p} a
    \]
    For any two vertices $u, v$ of $D$, define the quantity $\phi(u, v)$ to be
    \[
        \phi(u,v) = \sum_{p:u \rightarrow v} w(p),
    \]
    where $p: u \rightarrow v$ means that $p$ is a path from $u$ to $v$.

    Consider now the determinant 
    \[
        \det \begin{bmatrix}
            \phi(u_1, v_1) & \cdots & \phi(u_n, v_1) \\
             \vdots & \ddots & \vdots \\
            \phi(u_1, v_n) & \cdots & \phi(u_n, v_n)
        \end{bmatrix}
    \]
    \textcolor{red}{Finish this later.}
\end{theorem}

\subsubsection{Jacobi-Trudi statement and proof}

\begin{theorem} Let $\lambda$ be a partition. Then
    \[
        s_\lambda = \det(h_{\lambda_i+j-i})_{i,j=1}^{|\lambda|}.
    \]
\end{theorem}

\begin{proof}
    Consider the lattice, $\ZZ^2$. We assign weights to the edges so that going vertically, you pick up a $1$, and going horizontally at the height $i$, you pick up a $x_i$.

    We lay out the parts of $\lambda$ in ascending order at $x=1$.

    \textcolor{red}{Finish this later.}
\end{proof}

\section{The Robinson-Schensted-Knuth correspondence}

I'm going to define RSK functionally instead of imperatively. 

(After the fact, I think this was a waste of time and was more or less a cacophony of notation, but it was interesting regardless.)

\subsection{Row insertion}

\begin{definition}[Row insertion]
    Let $P = (P_{ij})$ be a SSYT of shape $\lambda$. And let $t \in \NN$. We will define the \textit{insertion path} $I(P\leftarrow t)$ now.
    We write that $t \geq P_m$ whenever $t \geq \max P_m$ or $P_m$ is empty and $t < P_m$ otherwise. 
    If $t < P_m$, define
    \begin{align*}
        (P_m \leftarrow t) := \begin{cases}
            \lambda_m + 1 & t \geq P_m \\
            \min\{n:P_{mn} > t\} & t < P_m 
        \end{cases}.
    \end{align*}
    We note that $(P_m \leftarrow t) \leq \lambda_m + 1$ always.
    We will define a recursive function $\RowInsert$ as follows.
    \begin{align*}
        &\RowInsert(P,m,t) \\
        &:= \begin{cases}
            [(P_m \leftarrow t)] & t \geq P_m \\
            [(P_m \leftarrow t)] + \RowInsert(P, m+1, P_{m(P_m \leftarrow t)}) & t < P_m
        \end{cases}.
    \end{align*}
    Where $+$ denotes list concatenation.
    Now, define $I(P\leftarrow t) := \RowInsert(P,1,t).$
\end{definition}

Now we define the tableau that results from row insertion, $(P \leftarrow t)$.

\begin{definition}
    Let $P$ be a SSYT and let $t \in \NN$. Let $I(P \leftarrow t) = [j_1, j_2, \ldots, j_M]$.
    \[
        (P \leftarrow t)_{rs} := \begin{cases}
            t & \text{if } (r,s) = (1, j_1) \\
            P_{m-1,j_{m-1}} & \text{if }(r,s) = (m, j_m) \\
            P_{rs} & \text{otherwise}.
        \end{cases}.
    \]
\end{definition}

We state a property of insertion paths that will allow us to prove that row insertion gives us SSYT.

\begin{theorem}
    Let $P$ be a SSYT of shape $\lambda$, let $t \in \PP$, and let $I(P\leftarrow t) = [j_1,j_2,\ldots,j_M]$. Then $I(P\leftarrow t)$ is weakly decreasing.
\end{theorem}

\begin{proof}
    Let $1 \leq m < M$.
    Suppose that $\lambda_{m+1} < j_m$. Then 
    \[
        j_{m+1} = (P_m \leftarrow k) \leq \lambda_m+1 \leq j_m.
    \]
    Now suppose that $\lambda_{m+1} \geq j_m$. 
    Then $P_{m+1,j_m}$ exists, and it must be that $P_{mj_m} < P_{m+1,j_m}$ due to column strictness. Then
    \[
        j_{m+1} = (P_{m+1} \leftarrow P_{mj_m}) = \min \{n:P_{m+1,n}>P_{mj_m}\} \leq j_m.
    \]
    Either way $j_{m+1} \leq j_m$.
\end{proof}

\begin{corollary}
    If $P$ is a SSYT and $t \in \PP$, then $(P \leftarrow t)$ is a SSYT.
\end{corollary}

\begin{proof}
    By the definition of row insertion, the rows are weakly increasing. Consider inserting a bumped number. By the previous theorem, it can only be moved down or down left, which means that it will always be inserted below a smaller number. This continues for the whole insertion path.
\end{proof}

\subsection{Generalized two-line notation}

\begin{definition}
    Let $A = (a_{ij})$ be a $\NN$-matrix. Let $i, j \in \NN$, and write a \textit{two-line entry} to be
    \[
        \binom{i}{j}.
    \]
    We concatenate two-line entries as follows
    \[
        \binom{i}{j} + \binom{k}{l} := \binom{i\:k}{j\:l}.
    \]
    Now consider the following two-line array:
    \[
        w_A = \sum_{i=1}^\infty \sum_{j=1}^\infty \binom{i}{j}^{a_{ij}}.
    \]
    If $A$ has finite support, $w_A$ is well-defined. 
    $w_A$ is the \textit{two-line array} or \textit{generalized permutation} corresponding to the matrix $A$.
\end{definition}

\begin{example}
    Let $A$ be the matrix
    \[
        \begin{bmatrix}
            1 & 0 & 3 & 2 \\
            0 & 1 & 0 & 0 \\
            2 & 4 & 1 & 0 \\
            0 & 0 & 1 & 0
        \end{bmatrix}.
    \]
    Then 
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 & 1 & 1 & 1 \
              & 2 \
              & 3 & 3 & 3 & 3 & 3 & 3 & 3\
              & 4 \\
            1 \
              & 2 & 2 & 2 \
              & 4 & 4 \
              & 2 \
              & 1 & 1 \
              & 2 & 2 & 2 & 2 \
              & 3 \
              & 3
        \end{pmatrix}
    \]
\end{example}

\subsection{The RSK algorithm}

\begin{definition}
    We define the \textit{RSK correspondence} to be a rule that assigns to $\NN$-matrices of finite support to pairs of SSYT $(P, Q)$, which we notate
    \[
        A \rskarrow (P, Q).
    \]
    To compute $(P, Q)$, let 
    \[
        w_A = \begin{pmatrix}
            i_1 & i_2 & \cdots & i_m \\
            j_1 & j_2 & \cdots & j_m
        \end{pmatrix},
    \]
    and begin with $(P_0, Q_0) = (\varnothing, \varnothing)$. Iteratively construct $(P_{t+1}, Q_{t+1})$ from $(P_t,Q_t)$ as follows
    \begin{enumerate}[label=(\alph*)]
        \item $P_{t+1} = P_t \leftarrow j_{t+1}$
        \item Add a new box to $Q_t$ where a new box to $P_t$ was added, and fill it with $i_{t+1}$.
    \end{enumerate}
    This process continues until $t=m$, and we take $(P, Q) = (P_m, Q_m)$.

    $P$ is referred to as the \textit{insertion tableau} and $Q$ is referred to as the \textit{recording tableau}.
\end{definition}

\begin{example}
    Let 
    \[
        A := \begin{bmatrix}
            1 & 0 & 2 \\
            0 & 2 & 0 \\
            1 & 1 & 0
        \end{bmatrix}.
    \]
\end{example}

\begin{theorem}
    RSK is a bijection between $\NN$-matrices $A$ of finite support and pairs of SSYT $(P,Q)$ such that
    \begin{align*}
        \type(P) &= \col(A), \\
        \type(Q) &= \row(A).
    \end{align*}
\end{theorem}

\end{document}
