\documentclass{article}

% See https://github.com/Jasper-Ty/dotfiles
\usepackage[garamond, tableau]{jaspercommon}
\usetikzlibrary{arrows.meta}

\DeclareMathOperator{\RSK}{RSK}
\DeclareMathOperator{\RowInsert}{RowInsert}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\shape}{shape}
\DeclareMathOperator{\sh}{sh}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\type}{type}
\DeclareMathOperator{\Par}{Par}
\DeclareMathOperator{\SYT}{SYT}
\DeclareMathOperator{\SSYT}{SSYT}
\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Lex}{Lex}
\DeclareMathOperator{\cyc}{cyc}
\DeclareMathOperator{\tr}{t}
\DeclareMathOperator{\s}{s}

\newcommand{\frkS}{\ensuremath\mathfrak{S}}
\newcommand{\rskarrow}{\ensuremath\xrightarrow{\RSK}}

\setcounter{MaxMatrixCols}{20}

\title{Symmetric Functions}
\author{Jasper Ty}
\date{}

\titleauthorhead

\begin{document}

\maketitle

\section*{What is this?}
These are notes based on my self-study of Chapter 7 in R.P Stanley's ``Enumerative Combinatorics'', mixed in with readings of various other expositions. 

I've learned to \textit{love} this subject! At first, I thought ``Functions that remain the same change under interchange of variables? What's so interesting about that?'', but at some point between now and the end of my undergraduate life, I took it on myself to \textit{compute} with these things, to hold them with my bare hands, and lo--- I suddenly found myself baptized in the waters of symmetric polynomials.

I'm not entirely sure how to write for an audience yet, so certain things might be over or under explained, and this might happen all over the place!
I guess, one has to have had some combinatorics, knowing about posets, partitions, coming up with bijections, and so on. 
Also experience with working with formal power series probably helps.

I'll be honest and say algebra is not my strong suit, so I apologize in advance if that manifests particularly clearly in some sections.

\tableofcontents

\section*{Notation and conventions}

(Yadda yadda.)

\addcontentsline{toc}{section}{Notation and conventions}

\subsection*{Sets}

We take $\NN$ to be the set of natural numbers \textit{including} zero,
\[
    \NN := \{0,1,2,\ldots\}.
\]
We take $\PP$ to be the set of \textit{positive integers},
\[
    \PP := \{1,2,\ldots\}.
\]
$\ZZ,\QQ,\RR,\CC$ are defined as usual.

\subsection*{Partitions and compositions}

A \textit{weak composition} $\alpha$ of $n \in \NN$ is an infinite tuple of nonnegative integers 
\[
    (\alpha_1, \alpha_2, \ldots)
\]
such that $\sum_i \alpha_i = n$. 
We define $|\alpha| = \sum_i \alpha_i$ to have notation for recovering $n$ given $\alpha$.

When more convenient (most of the time), we will omit brackets and commas, and write compositions as strings of digits. For example we will write $(1,1,4,3)$ as $1143$.

A \textit{partition} $\lambda$ of $n$ is a weak composition whose entries are \textit{weakly decreasing}. 
That a particular partition $\lambda$ is a partition of a particular $n$ is denoted $\lambda \vdash n$. 

I use English notation when drawing diagrams and tableaux, meaning, increasing row index means going north to south, and increasing column index means going east to west.

\subsection*{Rings, polynomials, and formal power series}

All rings considered are commutative and unital. An arbitrary ring will be denoted $\KK$. 

$\KK[[t]]$ will denote the formal power series ring over $\KK$ in the indeterminate $t$.

We will fix notation for the following sets of indeterminates:
\begin{enumerate}[label=(\alph*)]
    \item $X_N := (x_1, x_2, \ldots x_N)$ for a set of $N$ indeterminates.
    \item $X := (x_1, x_2, \ldots)$ for a set of countably many indeterminates.
    \item $Y$, $Y_N$, $Z$, $Z_N$, $Q$, $Q_N$ and so on are defined similarly.
\end{enumerate}

Let $\KK[[X]]$ be a formal power series ring. With compositions, partitions, or otherwise any finitely supported tuple of nonnegative integers $\alpha$, we define \textit{multi-index notation} for compactly writing down monomials in $\KK[[X]]$.
\[
    x^\alpha := x_1^{\alpha_1}x_2^{\alpha_2}x_3^{\alpha_3}\cdots.
\]
In the context of multi-index notation, $\alpha$ will also be called $x$'s \textit{exponent vector}.
We will let $[x^\alpha]f$ denote the coefficient of $[x^\alpha]$ in the formal power series $f$.

\subsection*{Permutations and the symmetric group}

$\frkS_n$ will denote the symmetric group on $n$ letters. I chose this symbol mostly because I think it looks \textit{\color{red} unbelievably} cool.

I will use one-line notation, generally, for permutations.

I use cycle notation, so e.g the cycle that sends $1$ to $7$, $7$ to $4$, and $4$ to $1$ will be written as $(174)$.

The simple transpositions $(i\:i+1)$ will be denoted $\s_i$.

Permutations will act on functions by permuting places, so if $w \in \frkS_n$ and $f$ is a function, then

\[
    wf(x_1,\ldots,x_n) = f(x_{w(1)},\ldots,x_{w(n)}).
\]

\section{Symmetric Functions}

And so it begins! We start with the now \textit{canonized} definition for ``the ring of symmetric functions''.

\subsection{As a certain subring}

More precisely, we construct the ring of symmetric functions as a certain subring of formal power series in infinitely many variables.
The reason for this mouthful is that we want a space into which all the facts we care about in the theory of symmetric \textit{polynomials} lift up.

Unfortunately, this means we \textit{have} to do some paperwork with regards to constructing this mythical ring.
\begin{definition}
    Fix a ground ring $\KK$.
    The ring $\KK[[X]]$ of \textit{formal power series in countably many variables} $X = (x_1, x_2, \ldots)$ is defined to be the set of all formal sums
    \[
        \sum_\alpha c_\alpha x^\alpha,
    \]
    where $c_a \in \KK$, and $\alpha$ ranges over all weak compositions, or equivalently over all finitely supported $\NN-$sequences.

    The ring operations are defined in the morally correct way;
    if $f = \sum_\alpha a_\alpha x^\alpha$ and $g = \sum_\alpha b_\alpha x^\alpha$, then $f + g$ and $fg$ are defined to be
    \[
        f+g := \sum_\alpha (a_\alpha+b_\alpha)x^\alpha
    \]
    and
    \[
        fg := \sum_\gamma \prod_{\alpha+\beta=\gamma} (a_\alpha b_\beta) x^\gamma.
    \]
\end{definition}

Indexing over all \textit{weak compositions} $\alpha$ means that all the monomials that appear are more or less ``honest'', and this makes the definition work nicely with our existing intuition for working with formal power series.

\begin{example}
    The formal power series
    \[
        f(X) = x_1 + x_2 + x_3 + \cdots
    \]
    is an element of $\KK[[X]]$.
\end{example}

$\KK[[X]]$ contains as subrings, in an extremely direct way, every power series ring in \textit{finitely many variables} $\KK[[x_{i_1}, \ldots, x_{i_n}]]$.

On this level, it is already some kind of generalization of working with finitely many variables.
Next, we add another ingredient which generalizes, in a similar way, what we mean when we say ``symmetric''.

\begin{definition}
    Let $\frkS_\infty$ denote the subgroup of $\frkS_\NN$ consisting of permutations of $\NN$ with ``finite support''.
    That is,
    \[
        \frkS_\infty := \{w \in \frkS_\NN: w(t) = t\text{ for all but finitely many }t\}.
    \]
\end{definition}

Again, this contains as subgroups every $\frkS_n$ for all $n \in \NN$.
Moreover, it's not hard to see that every permutation in $\frkS_\infty$ is an extension of a permutation that lives in some finite symmetric group.

\subsubsection{Homogeneous symmetric functions}

We're ready to start defining the ring of symmetric functions!

\begin{definition}
    A \textit{homogeneous symmetric function of degree $n$} over a ring $\KK$ is a formal power series
    \[
        \sum_\alpha c_\alpha x^\alpha \in \KK[[X]],
    \]
    where we are summing over all weak compositions $\alpha$ of $n$, and every $c_\alpha$ is a scalar such that $c_\alpha = c_\beta$ whenever $\beta$ can be obtained by permuting the parts of $\alpha$.
    
    We denote the set of all such formal power series by $\Lambda_\KK^n$.
\end{definition}

These form a $\KK$-module, as a submodule of $\KK[[X]]$.
Moreover, these are in fact defined correctly, meaning that these are symmetric ``functions''.

\begin{remark}
    Let $w \in \frkS_\infty$ and let $\phi_w: K[[X]] \to K[[X]]$ be the $\KK$-algebra isomorphism given by sending each indeterminate $x_i$ to $x_{w(i)}$.
    Then if $f(X) \in \Lambda_\KK^n$, $f(X)$ is a fixed point of $\phi$.
\end{remark}

The following is a simple example of an element of $\Lambda_\KK^n$:

\begin{example}
    The formal power series 
    \[
        f = \sum_i x_i^2 + 10\sum_{i < j} x_ix_j
    \]
    is a symmetric function that is homogeneous of degree $2$. In this case $c_\alpha = 1$ whenever $\alpha = \ldots 2 \ldots$, and $c_\alpha = 10$ whenever $\alpha = \ldots 1 \ldots 1 \ldots$. 
\end{example}


We note that multiplying any two homogeneous symmetric functions $f$, $g$ of degree $m$ and $n$ respectively give us a homogeneous symmetric function of degree $m+n$.
The following definition gives us the right subalgebra this remark hints at.

\subsubsection{Symmetric functions}

\begin{definition}
    The \textit{ring of symmetric functions} $\Lambda_\KK$ is the infinite direct sum
    \[
        \Lambda_\KK := \Lambda_\KK^0 \oplus\Lambda_\KK^1 \oplus \cdots.
    \]
    In the case when $\KK = \QQ$, we will suppress $\QQ$ and refer to $\Lambda_\QQ^n$ and $\Lambda_\QQ$ as $\Lambda^n$ and $\Lambda$ respectively.
\end{definition}

This direct sum is \textit{internal}, happening within $\KK[[X]]$. 

This greatly broadens the possible definitions for symmetric functions. 
For example, the following demonstrates a symmetric function that arises from an infinite product, which evidently contains many monomials of different degrees and is not at all homogeneous.

\begin{example}
    The formal power series 
    \[
        f = \prod_i (1 + 3x_i^2 + 7x_i^5)
    \]
    is a symmetric function.
\end{example}

And in fact, we will find many such symmetric functions.

\subsection{As an inverse limit of graded rings}

The \textit{definition} of a symmetric function arises from the study of \textit{symmetric polynomials}. Specifically, it is a limit in the category of graded rings.

Consider the polynomial ring $K = \KK[x_1,\ldots,x_N]$. We denote the subring of $K$ which consists of all polynomials $f(x_1,\ldots,x_N)$ such that $wf = f$ for all $w \in \frkS_N$ to be the \textit{ring of symmetric polynomials in $N$ variables}.
We denote this subring by $\KK[x_1,\ldots,x_N]^{\frkS_N}$.

\begin{example}
    The polynomial
    \[
        f(x) = x_1^2x_2 + x_1x_2^2
    \]
    lies inside $\KK[x_1,x_2]^{\frkS_2}$.
\end{example}

Define $\Lambda_n = \QQ[x_1,\ldots,x_n]^{\frkS_n}$.

Clearly, 

\section{Partitions}

Partitions are evidently extremely important in the theory of symmetric functions for reasons that are, honestly, still quite mysterious to me.

\subsection{Definition}

A partition is just a way of writing down $n$ as a sum of positive integers.

\begin{definition}
    A partition $\lambda$ of $n \in \NN$, which we denote $\lambda \vdash n$, is a sequence of numbers 
    \[
        (\lambda_1,\lambda_2,\ldots)
    \]
    such that 
    \[
        \sum_{i\in\NN}\lambda_i = n
    \]
    and 
    \[
        \lambda_j \leq \lambda_k
    \]
    for all $j\geq k$.
\end{definition}

In other words, $\lambda_k$ is weakly decreasing and has only finitely many nonzero entries.

\begin{example}
    The sequence
    \[
        (\lambda_i) = (5,4,4,3,2,1,1,1,0,\ldots)
    \]
    is a partition of $21$.
\end{example}

We will truncate the trailing zeros and elide tuple notation when convenient (most of the time), so the above partition is also written $54432111$.

The set of all partitions $\lambda$ such that $\lambda \vdash n$ is denoted $\Par(n)$. We define the set of \textit{all} partitions to be just $\Par$.

\subsection{Diagrams}
Partitions can be drawn as \textit{Ferrers diagrams} and \textit{Young diagrams}. 

Both have the same underlying data structure: they encode the partition $\lambda$ as a subset of $\NN^2$, with a particularly simple definition:

\begin{definition}
    Let $\lambda \vdash n$. The \textit{diagram} of $\lambda$ is the set 
    \[
        \{(i,j) \in \NN^2 : 1 \leq j \leq \lambda_i\}.
    \]
\end{definition}

Then, we can define Young and Ferrers diagrams.

\begin{definition}
    Let $\lambda \vdash n$.
    The \textit{Young diagram} of $\lambda$ is obtained by drawing a box at location $(i,j)$ for each $(i,j)$ in $\lambda$'s diagram.
    Similarly, its \textit{Ferrers diagram} is obtained by plotting dots rather than boxes.
\end{definition}
\begin{example}
    The Young diagram of the partition $\lambda = 54432111$ is
    \[
        \ydiagram{5,4,4,3,2,1,1,1}.
    \]
\end{example}
\begin{example}
    The Ferrers diagram of the partition $\lambda = 54432111$ is
    \[
        \begin{matrix}
            . & . & . & . & . \\
            . & . & . & . &  \\
            . & . & . &   &  \\
            . & . &   &   &  \\
            . &   &   &   &  \\
            . &   &   &   &  \\
            . &   &   &   & 
        \end{matrix}
    \]
\end{example}

\subsection{Tableaux}

The fact that Young diagrams are made up of boxes is nice--- because we can put things in the boxes! 

\begin{definition}
    Let $\lambda \vdash n$. A Young diagram of $\lambda$ whose boxes are filled in with elements from a set is called a \textit{Young tableau}, which will often be denoted with a capital letter, say $T$. 

    $\lambda$ is referred to as the \textit{shape} of the tableau, denoted $\shape T$ or $\sh T$.

    The elements filled in are called the \textit{entries} of the tableau, and the entry at box $(i,j)$ is indexed as $T_{ij}$.
\end{definition}

\begin{example}
    The following is a Young tableau, filled in with positive integers:
    \[
        \ytableaushort{123,45}
    \]
\end{example}

More formally, Young tableau are functions whose domain is a partition's diagram.
A partition's diagram has an order induced on it by being a subset of $\NN^2$--- consider the product order on $\NN^2$. 
\textit{Magic starts to happen} when you consider a function on $\NN^2$ whose codomain is also a poset, say $\NN$.

Well, really, this just means a Young diagram with numbers filled in the boxes, but this is a fertile ground for having $\NN^2$'s order interact with $\NN$'s order!

\begin{definition}
    We define a few important constraints on a Young tableau $T$.
    \begin{enumerate}[label=(\alph*)]
        \item That the \textit{rows} of $T$ are \textit{weakly increasing} means that
            \[
                T_{n,i} \leq T_{n,j} \textit{ for all } i < j.
            \]
        \item That the \textit{columns} of $T$ are \textit{weakly increasing} means that
            \[
                T_{i,m} \leq T_{j,m} \textit{ for all } i < j.
            \]
        \item That the \textit{rows} of $T$ are \textit{strongly increasing} means that
            \[
                T_{n,i} < T_{n,j} \textit{ for all } i < j.
            \]
        \item That the \textit{columns} of $T$ are \textit{strongly increasing} means that
            \[
                T_{i,m} < T_{j,m} \textit{ for all } i < j.
            \]
    \end{enumerate}
\end{definition} 

These have really obvious meanings on the level of ``filling numbers in boxes'' (try it!).

With this, we can define two important classes of Young tableau.

\begin{definition}
    A Young tableau $T$ is called \textit{standard} if both its rows and columns are strongly increasing. $T$ is called \textit{semistandard} if its rows are weakly increasing and its columns are strongly increasing.

    Let $\lambda \vdash n$. We denote the set of all standard Young tableau of shape $\lambda$ by $\SYT(\lambda)$, and the set of all semistandard Young tableau by $\SSYT(\lambda)$.
\end{definition}

Semistandard Young tableau are explored in more detail in Section \ref{ch:schurs}.

\subsection{Orders on partitions}

We have several orders on \textit{partitions themselves}.
The first one, \textit{containment}, is defined on all partitions.

\begin{definition}[Containment order]
    Young diagrams, as subsets of $\NN^2$, have a partial order induced by containment. 
    \textit{Containment order} for partitions is precisely this order that diagrams induce on partitions.

    We will use $\subseteq$ to denote this order.
\end{definition}

Containment order in fact induces a lattice, a sublattice of $\NN^2$'s powerset, once it's checked that $\Par$ is closed under union and intersection. 

\begin{definition}
    The lattice structure on $\Par$ given by containment is called \textit{Young's lattice}.
\end{definition}

Containment is somewhat sensitive to partition size--- for $\lambda \subseteq \mu$ it's necessary that $|\lambda| \leq |\mu|$.
Even more sharply, all partitions of a fixed size are incomparable in Young's lattice!

The next two orders are not so sensitive to a partition's size, and they arise not from viewing partitions as diagrams, but viewing them as sequences.

\begin{definition}[Dominance order]
    Let $\lambda$ and $\nu$ be two partitions. We say that $\lambda$ \textit{dominates} or \textit{majorizes} $\nu$ if
    \[
        \sum_{k=1}^i \lambda_k \geq \sum_{k=1}^i \nu_k \qquad \forall i\in\NN.
    \]
    We denote this relation $\lambda \leq \nu$.
\end{definition}

\begin{theorem}
    $\lambda$ covers $\nu$ if
\end{theorem}

\begin{definition}[Lexicographic order]
    We define the \textit{lexicographic order} on $\Par$ to be

    We denote this relation $\lambda \prec \nu$.
\end{definition}

\begin{theorem}[Lexicographic order is a total order]
\end{theorem}

\begin{theorem}[Dominance order embeds into lexicographic order]
\end{theorem}

\section{Distinguished bases of symmetric functions}

We will cover several interesting bases of the ring of symmetric functions. These are \textit{all} indexed by partitions.

\subsection{Monomial symmetric functions}

The first basis, the monomials, has the amazing property where it's \textit{remarkably obvious the fact that it even is a basis}.

This is in total analogy to taking the \textit{monomial basis of a polynomial ring}. In this case, we group together monomials by the orbits of the place-permuting $\frkS_n$ action.

\begin{definition}
    Let $\lambda \vdash n$. The \textit{monomial symmetric function} $m_\lambda$ is 
    \[
        m_\lambda := \sum_{\alpha \sim \lambda} x^\alpha.
    \]
    where $\alpha \sim \lambda$ means that $\alpha$ may be obtained by permuting the parts of $\lambda$.
\end{definition}

The above definition involves permuting around the exponent vector $\alpha$. I personally find it actually much easier to think of the monomial symmetric functions as \textit{permuting the subscripts}. 

\begin{example}
    Let $\lambda = 5322$. Then
    \begin{align*}
        m_\lambda = m_{5322} = \sum_{i_1<i_2<i_3<i_4} \Big(&x_{i_1}^5x_{i_2}^3x_{i_3}^2x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^3x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^2x_{i_4}^3 \\ 
            + &x_{i_1}^2x_{i_2}^5x_{i_3}^3x_{i_4}^2 + x_{i_1}^2x_{i_2}^5x_{i_3}^2x_{i_4}^3 + x_{i_1}^2x_{i_2}^2x_{i_3}^5x_{i_4}^3  \\
            + &x_{i_1}^3x_{i_2}^5x_{i_3}^2x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^5x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^2x_{i_4}^5  \\
        + &x_{i_1}^2x_{i_2}^3x_{i_3}^5x_{i_4}^2 + x_{i_1}^2x_{i_2}^3x_{i_3}^2x_{i_4}^5 + x_{i_1}^2x_{i_2}^2x_{i_3}^3x_{i_4}^5 \Big).
    \end{align*}
    when you view the action of $\frkS_n$ as permuting the exponents. When viewed as permuting the subscripts, we have that
    \begin{align*}
        m_\lambda = m_{5322} = \sum_{i_1<i_2<i_3<i_4} \Big(
        &x_{i_1}^5x_{i_2}^3x_{i_3}^2x_{i_4}^2
        + x_{i_1}^5x_{i_3}^3x_{i_2}^2x_{i_4}^2
        + x_{i_1}^5x_{i_4}^3x_{i_2}^2x_{i_3}^2 \\
        + &x_{i_2}^5x_{i_1}^3x_{i_3}^2x_{i_4}^2
        + x_{i_2}^5x_{i_3}^3x_{i_1}^2x_{i_4}^2
        + x_{i_2}^5x_{i_4}^3x_{i_1}^2x_{i_3}^2 \\
        + &x_{i_3}^5x_{i_1}^3x_{i_2}^2x_{i_4}^2
        + x_{i_3}^5x_{i_2}^3x_{i_1}^2x_{i_4}^2
        + x_{i_3}^5x_{i_4}^3x_{i_2}^2x_{i_4}^2 \\
        + &x_{i_4}^5x_{i_1}^3x_{i_2}^2x_{i_3}^2
        + x_{i_4}^5x_{i_2}^3x_{i_1}^2x_{i_3}^2
        + x_{i_4}^5x_{i_3}^3x_{i_1}^2x_{i_2}^2
    \Big).
    \end{align*}
\end{example}

\begin{theorem}
    The monomial symmetric funtions form a basis for $\Lambda$.
\end{theorem}

\begin{proof}
    It's impossible to form a nontrivial linear combination of monomial symmetric functions that sum to zero.
\end{proof}

\subsection{Elementary symmetric functions}

Our next basis will be the \textit{elementary symmetric functions}, which we will refer to as the \textit{elementaries} or the $e$'s.

\begin{definition}
    Let $n\in\NN$. The \textit{elementary symmetric function} $e_n$ is defined to be
    \[
        e_n := \sum_{i_1<i_2<\ldots<i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $e_\lambda$ is defined to be
    \[
        e_\lambda := e_{\lambda_1}e_{\lambda_2}\cdots.
    \]
\end{definition}

Now we examine the relationship between the elementaries and the monomials.

\begin{definition}
    Let $\lambda \vdash n$. We define $M_{\lambda\mu}$ to be the coefficient of $m_\mu$ in the expansion of $e_\lambda$ in the monomial basis. That is, the numbers such that
    \[
        e_\lambda = \sum_\mu M_{\lambda\mu} m_\mu.
    \]
    More generally, for any weak composition $\alpha$, let $M_{\lambda\alpha}$ be the coefficient of $x^\alpha$ in $e_\lambda$.
\end{definition}

\begin{theorem} \label{thm:e2mCombInterpretation}
    The coefficient $M_{\lambda\mu}$ is counted by zero-one matrices whose row-sums are $\lambda$ and whose column sums are $\mu$.
\end{theorem}

\begin{proof}

    Consider what is going on when we compute the terms of $e_\lambda$,
    \[
        e_\lambda = e_{\lambda_1}e_{\lambda_2}\ldots.
    \]
    The right hand side is
    \[
        \left(\sum_{i_1<\ldots<i_{\lambda_1}} x_{i_1}\cdots x_{i_{\lambda_1}}\right)\left(\sum_{i_1<\ldots<i_{\lambda_2}} x_{i_1}\cdots x_{i_{\lambda_2}}\right)\cdots
    \]
    and naming a term in this product means you pick up $\lambda_1$ distinct variables from $e_{\lambda_1}$, $\lambda_2$ distinct variables from $e_{\lambda_2}$, and so on. 
    
    These choices of distinct variables are $\lambda_i$-sized subsets of $(x_1, x_2, \ldots)$, and encoding these subsets with lists of $1$s and $0$s gives us the rows of our $0-1$ matrix, where the row $i$ corresponds to $e_{\lambda_i}$.     
    \[
        \begin{bmatrix}
            x_1 & x_2 & x_3 & \cdots \\
            x_1 & x_2 & x_3 & \cdots \\
            x_1 & x_2 & x_3 & \cdots \\
            \vdots & \vdots & \vdots & \ddots 
        \end{bmatrix}
    \]
    The fact that we picked up $\lambda_i$ variables in each row manifests as the $i$-th row sum being equal to $\lambda_i$.
The exponent of a given variable $x_j$ appearing in a monomial only depends on how many times we picked up an $x_j$ from each $e_{\lambda_i}$. 
This manifests as the $j$-th column sum being equal to $\mu_i$.


\end{proof}

\begin{theorem} \label{thm:e2msymmetric}
    Let $\lambda,\mu \vdash n$, then
    \[
        M_{\lambda\mu} = M_{\mu\lambda}.
    \]
\end{theorem}

\begin{proof}
    Matrix transposition is a bijection between the sets the two numbers count.
\end{proof}

\subsubsection{The fundamental theorem of symmetric functions}
\begin{theorem}[Gale-Ryser] \label{thm:galeryser} Let $M = [a_{ij}]$ be a zero-one matrix, whose row sums are given by the composition $\alpha$ and whose column sums are given by composition $\beta$. Then it must be that $\alpha \leq \beta^T$. Moreover, there is only \textit{one} zero-one matrix such that $\alpha = \beta^T$.
\end{theorem}

\begin{proof}
    We demonstrate this algorithmically. 
    Let
\end{proof}

\begin{theorem}[Fundamental theorem of symmetric functions] The $e$'s form a $\ZZ$-basis for the ring of symmetric functions.
\end{theorem}

\begin{proof}
    By Theorem \ref{thm:galeryser}, the transition matrix is upper-triangular and has $1$'s on the diagonal, hence it is invertible. 
\end{proof}

\subsection{Complete homogeneous symmetric functions}

The \textit{complete homogeneous symmetric functions}, or the \textit{completes}, or the $h$'s, have a very similar definition as the elementaries, but with distinctness relaxed.

\begin{definition}
    Let $n\in\NN$. The \textit{complete homogeneous symmetric function} $h_n$ is defined to be
    \[
        h_n := \sum_{i_1\leq i_2\leq\ldots\leq i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $h_\lambda$ is defined to be
    \[
        h_\lambda := h_{\lambda_1}h_{\lambda_2}\cdots.
    \]
\end{definition}

As with the elementaries and $M_{\lambda\mu}$, we define a set of monomial coefficients for the completes.

\begin{definition}
    Let $\lambda \vdash n$. Define $N_{\lambda\alpha}$ be the coefficient of $x^\alpha$ in $h_\lambda$.
\end{definition}

These numbers satisfy analogous theorems.

\begin{theorem} \label{thm:h2mCombInterpretation}
    Let $\lambda, \mu$ be partitions. Then $N_{\lambda\mu}$ is counted by $\NN$-matrices with row sums $\lambda$ and column sums $\mu$.
\end{theorem}

\begin{theorem} \label{thm:h2mSymmetric}
    Let $\lambda, \mu \vdash n$, then
    \[
        N_{\lambda\mu} = N_{\mu\lambda}.
    \]
\end{theorem}

\begin{proof}[Proof of Theorems \ref{thm:h2mSymmetric} and \ref{thm:h2mCombInterpretation}]
    These are proved almost exactly the same way as Theorems \ref{thm:e2mCombInterpretation} and \ref{thm:e2msymmetric}.
\end{proof}

However, we do not have an analogue of Theorem \ref{thm:galeryser} for $N_{\lambda\mu}$.
\subsection{Power sum symmetric functions}

We have one more simple basis for $\Lambda$, which has many not-so-simple theorems!

\begin{definition}
    Let $n\in\NN$. The \textit{power sum symmetric function} $p_n$ is defined to be
    \[
        p_n := \sum_{i} x_i^n
    \]
    And if we let $\lambda \vdash n$, the elementary symmetric function $p_\lambda$ is defined to be
    \[
        p_\lambda := p_{\lambda_1}p_{\lambda_2}\cdots.
    \]
\end{definition}

\begin{definition}
    Let $\lambda \vdash n$. Let $R_{\lambda\alpha}$ be the coefficient of $x^\alpha$ in $p_\lambda$.
\end{definition}

\begin{theorem}
    The coefficient $R_{\lambda\mu}$ occurring in the monomial expansion of $p_\lambda$,
    \[
        p_\lambda = \sum_\mu K_{\lambda\mu} m_\mu
    \]
    is counted by the number of ordered partitions $\pi = (B_1, \ldots, B_k)$, where $k = \ell(\mu)$, such that
    \[
        \mu_i = \sum_{j\in B_i} \lambda_j.
    \]
\end{theorem}

\begin{proof}
    Choosing a term in $p_\lambda$ means picking up an $x_{i_j}^{\lambda_j}$ term from each
    \[
        p_{\lambda_j} = (\cdots + x_{i_j}^{\lambda_j} + \cdots).
    \]
    Evidently, given such a choice, we can partition the $\lambda_j$'s into subsets which pick out the same indeterminate $x_{i_j}$, and this subset determines the degree of $x_{i_j}$ in our monomial.
\end{proof}

\subsubsection{Cycle type}

\begin{definition}
    Let $w \in \frkS_n$. The \textit{cycle type} $\rho(w)$ of $w$ is the partition of $n$ whose parts are the cycle lengths of $w$'disjoint cycle decomposition.
\end{definition}

\begin{example}
    The cycle type of $w = 1657234$ is $\rho(w) = 421$, since $w$ factorizes as
    \[
        w = (2635)(47)(1).
    \]
\end{example}

\begin{definition}
    Define the number $z_\lambda$ to be
    \[
        z_\lambda := 1^{m_1}m_1!2^{m_2}m_2!\cdots.
    \]
\end{definition}

This is important when it comes to enumeration with regards to cycle type.

\begin{theorem}
    The number of permutations $w \in \frkS_n$ of cycle type $\rho = \langle 1^{m_1}2^{m_2}\cdots\rangle$ is
    \[
        \frac{n!}{1^{m_1}m_1!2^{m_2}m_2!\cdots} = n!z_\rho^{-1}.
    \]
\end{theorem}

\begin{proof}
    Split off the denominator into two, so that we have
    \[
        \frac{n!}{(1^{m_1}2^{m_2}\cdots)(m_1!m_2!\cdots)}.
    \]
    The left accounts for an ``internal'' symmetry, that of \textit{permuting the insides of each cycle}. 
    The right accounts for an ``external'' symmetry, that of \textit{permuting the cycles themselves}.
    Specifically, take a permutation $w = w_1w_2\cdots w_n$, viewing it only as a tuple of numbers, i.e a word.
    There are $n!$ many such $w$.
    We may construct a permutation $[w]$ out of $w$ with cycle type $\rho$ by considering the permutation
    \[
        (w_1\cdots w_{\rho_1})
        (w_{\rho_1+1}\cdots w_{\rho_1+\rho_2})\cdots(w_{\rho_1+\cdots+\rho_{k-1}+1}\cdots w_{\rho_1+\cdots+\rho_{k-1}+\rho_k}).
    \]
    The group action 
\end{proof}

\section{Power series identities}

We define the following generating functions in $\Lambda[[t]]$.
\begin{definition}
    \begin{align*}
        H(t) &:= \sum_{n\in\NN}h_n t^n \\
        E(t) &:= \sum_{n\in\NN}e_n t^n \\
        P(t) &:= \sum_{n\geq1}p_n t^n
    \end{align*}
\end{definition}

Note that $P(t)$ has constant term zero, while $E(t)$ and $H(t)$ have constant term one!

\begin{theorem}\label{thm:HEPPowerSeries}
    We have that
    \begin{align*}
        H(t) &= \prod_{n\in\NN}\frac{1}{1-x_nt}, \\
        E(t) &= \prod_{n\in\NN}(1+x_nt), \\
        P(t) &= \sum_{n\in\NN}\frac{x_nt}{1-x_nt}.
    \end{align*}
\end{theorem}

\begin{proof}
    \begin{align*}
        \prod_{n\in\NN}\frac{1}{1-x_nt} &= \prod_{n\in\NN}\Big(1 + x_nt + x_n^2t^2 + \cdots\Big) \\
                                        &= \sum
    \end{align*}
\end{proof}

\subsection{The Newton-Girard formulas}

\begin{theorem}[Newton-Girard formulas]
    Let $n \in \PP$. Then
    \begin{align}
        \sum_{k=0}^n (-1)^k e_kh_{n-k} &= 0 \label{ng1} \\
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k &= ne_n  \label{ng2} \\
        \sum_{k=0}^n h_{n-k}p_k &= nh_n \label{ng3}
    \end{align}
\end{theorem}

\begin{proof}
    These all follow from Theorem \ref{thm:HEPPowerSeries}. We have that
    \[
        H(t)E(-t) 
        = \left(\prod_{n\in\NN}\frac{1}{1-x_nt}\right)\left(\prod_{n\in\NN}1-x_nt\right) 
        = \prod_{n\in\NN}\frac{1-x_nt}{1-x_nt} = 1.
    \]
    Then $[t^n] H(t)E(-t) = 0$ for all $n \geq 1$, giving us
    \[
        \sum_{k=0}^n (-1)^k e_k h_{n-k} = 0 \qquad \forall n\geq 1.
    \]
    This proves the first Newton-Girard formula \eqref{ng1}.

    Then, we have that
    \begin{align*}
        E(-t)P(t) &= \left[\prod_{n\in\NN}(1-x_nt)\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                  &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}(1-x_nt)\right] \\
                  &= t\sum_{m\in\NN} \left[x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right] \\
                  &= -t\sum_{m\in\NN} \left[-x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right].
    \end{align*}
    The sum can be expressed as the derivative of an infinite product, and we can continue the simplification
    \begin{align*}
                  &= -t\frac{d}{dt}\left[\prod_{m\in\NN}(1-x_mt)\right] \\
                  &= -t\frac{d}{dt}E(-t) \\
                  &= -t\frac{d}{dt}\left[\sum_{n\in\NN}(-1)^n e_nt^n\right] \\
                  &= -t\left[\sum_{n\geq 1}n(-1)^n e_nt^{n-1}\right] \\
                  &= \sum_{n\geq 1}n(-1)^{n-1} e_nt^n.
    \end{align*}
    Then, the formula for $[t^n]E(-t)P(t)$ given $n \geq 1$ is
    \[
        \sum_{k=0}^n (-1)^k e_{n-k}p_k = n(-1)^{n-1}e_n \qquad \forall n \geq 1.
    \]
    And after moving the $-1$ factors,
    \[
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k = ne_n.
    \]
    This proves the second Newton-Girard formula, \eqref{ng2}.

    The proof of the third is very similar and actually even easier, since
    \begin{align*}
        \frac{d}{dt}H(t) &= \frac{d}{dt}\left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right] \\
                         &= \sum_{m\in\NN}\left[\frac{x_m}{(1-x_mt)^2}\prod_{\substack{n\in\NN \\ n \neq m}}\frac{1}{1-x_nt}\right] \\ 
                         &= \sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}\frac{1}{1-x_nt}\right]
    \end{align*}
    Then
    \begin{align*}
        H(t)P(t) &= \left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                 &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod{n\in\NN}\frac{1}{1-x_nt}\right] \\
                 &= t\frac{d}{dt}H(t) \\
                 &= t\frac{d}{dt}\left[\sum_{n\in\NN}h_nt^n\right] \\
                 &= t\sum_{n\geq 1}nh_nt^{n-1} \\
                 &= \sum_{n\geq 1}nh_nt^n,
    \end{align*}
    which proves the third Newton-Girard formula, \eqref{ng3}.
\end{proof}

\subsection{Vieta's formulas}

\subsection{Cauchy identities}

\begin{theorem}\label{CauchyIdentityForME}
    We have that
    \[
        \prod_{i,j \geq 1} (1+x_iy_j) = \sum_{\lambda\in\Par} m_\lambda(X) e_\lambda(Y).
    \]
\end{theorem}

\begin{proof}
    The coefficient of $x^\alpha y^\beta$ is obtained by taking a $1,0-$matrix $A = (a_{ij})$ whose row sum is $\alpha$ and whose column sum is $\beta$. Then
    \begin{align*}
        \prod_{i,j\geq1}(1+x_iy_j) &= \sum_{A:1,0-\text{matrix}}x^{\row A}y^{\col A} \\
                                   &= \sum_{\lambda,\mu} M_{\lambda\mu} (m_\lambda(X) m_\mu(Y)) \\
                                   &= \sum_{\lambda} m_\lambda(X) \left[\sum_\mu M_{\lambda\mu} m_\mu(Y)\right]\\
                                   &= \sum_\lambda m_\lambda(X) e_\lambda(Y).
    \end{align*}
\end{proof}

\begin{theorem}\label{CauchyIdentityForMH}
    We have that
    \[
        \prod_{i,j \geq 1} (1-x_iy_j)^{-1} = \sum_{\lambda\in\Par} m_\lambda(X) h_\lambda(Y).
    \]
\end{theorem}

\begin{proof}
    Exactly the same as with Theorem \ref{CauchyIdentityForME}.
    \begin{align*}
        \prod_{i,j\geq1}(1+x_iy_j) &= \sum_{A:\NN-\text{matrix}}x^{\row A}y^{\col A} \\
                                   &= \sum_{\lambda,\mu} N_{\lambda\mu} (m_\lambda(X) m_\mu(Y)) \\
                                   &= \sum_{\lambda} m_\lambda(X) \left[\sum_\mu N_{\lambda\mu} m_\mu(Y)\right]\\
                                   &= \sum_\lambda m_\lambda(X) h_\lambda(Y).
    \end{align*}
\end{proof}

\begin{theorem}\label{CauchyIdentityForPP}
    We have that
    \[
        \prod_{i,j \geq 1} (1-x_iy_j)^{-1} = \sum_{\lambda\in\Par} z_\lambda^{-1}p_\lambda(X) p_\lambda(Y).
    \]
\end{theorem}

\section{\texorpdfstring{$\omega$}{w}-involution}


\begin{theorem}
    For all $n$,
    \[
        \omega(h_n) = e_n.
    \]
    Thus, $\omega$ is an involution.
\end{theorem}

\begin{proof}
    We will use the first Newton-Girard formula \eqref{ng1}.
\end{proof}

\section{The Hall inner product}

We define an inner product $\langle \cdot, \cdot \rangle$ on $\Lambda$ via the following rule:

\begin{definition} Let $\langle\cdot,\cdot\rangle$ be the scalar product defined by the relationship
    \[
        \langle m_\lambda, h_\nu \rangle = \delta_{\lambda\nu}.
    \]
    This scalar product is called the \textit{Hall inner product}. It is well defined since the $m$'s and $h$'s form a basis for $\Lambda$.
\end{definition}

\begin{theorem}
    $\langle\cdot,\cdot\rangle$ is symmetric.
\end{theorem}

\begin{proof}
    It suffices to prove that products of basis elements are symmetric for some bases of $\Lambda$. Consider the basis $\{h_\lambda\}$. We have that
    \[
        \langle h_\lambda, h\nu \rangle = \left\langle \sum_{\gamma} N_{\lambda\gamma} m_\gamma, h\nu \right\rangle = N_{\lambda\nu}.
    \]
    Then $\langle h_\lambda, h_\nu \rangle = N_{\lambda\nu} = N_{\nu\lambda} = \langle h_\lambda, h_\nu \rangle$.
\end{proof}

\begin{theorem}\label{thm:CauchyIdentityImpliesOrthonormal}
    Any two bases $\{u_\lambda\}$ and $\{v_\lambda\}$ of $\Lambda$ that have a Cauchy identity
    \begin{equation}\label{eq:CauchyIdentityArbitrary}
        \prod_{i,j \geq 1}(1 - x_iy_j)^{-1} = \sum_{\lambda\in\Par} u_\lambda(X) y_\lambda(Y)
    \end{equation}
    are orthonormal with respect to the Hall inner product.
\end{theorem}

\begin{proof}
    Let
    \[
        m_\lambda = \sum_\rho \zeta_{\lambda\rho} u_\rho, \qquad h_\mu = \sum_\nu \eta_{\mu\nu} v_\nu.
    \]
    Then
    \[
        \delta_{\lambda\mu} = \langle m_\lambda, h_\mu \rangle = \sum_{\rho,\nu}\zeta_{\lambda\rho} \eta_{\mu\nu} \langle u_\rho, u_\nu \rangle
    \]
    If $\langle u_\lambda, v_\mu \rangle = \delta_{\lambda\mu}$, the above becomes
    \[
        \delta_{\lambda\mu} =  \sum_{\rho,\nu}\zeta_{\lambda\rho} \eta_{\mu\nu} \delta_{\rho\nu}.
    \]
    Since $\sum_\nu \eta_{\mu\nu}\delta_{\rho\nu} = \eta_{\mu\rho}$, we find that
    \begin{equation}\label{eq:CauchyIdentityImpliesDualBasis:1}
        \delta_{\lambda\mu} =  \sum_\rho\zeta_{\lambda\rho} \eta_{\mu\rho}.
    \end{equation}
    The sequence of equalities given can be run backwards, so we have to just show (\ref{eq:CauchyIdentityImpliesDualBasis:1}) and the theorem is proven.
    By Theorem \ref{CauchyIdentityForMH}, we have that
    \[
        \prod_{i,j}(1-x_iy_j)^{-1} = \sum_\lambda m_\lambda(X) h_\lambda(Y).
    \]
    So 
    \[
        \prod_{i,j}(1-x_iy_j)^{-1} = \sum_\lambda \left(\sum_\rho \zeta_{\lambda\rho} u_\rho(X)\right)\left(\sum_\nu \eta_{\lambda\nu}v_\nu(Y)\right).
    \]
    Interchanging sums,
    \[
        \sum_\lambda \left(\sum_\rho \zeta_{\lambda\rho} u_\rho(X)\right)\left(\sum_\nu \eta_{\lambda\nu}v_\nu(Y)\right) = \sum_{\rho,\nu} \left(\sum_\lambda \zeta_{\lambda\rho} \eta_{\lambda\nu}\right) u_\rho(X) v_\nu(Y).
    \]
    By (\ref{eq:CauchyIdentityArbitrary})
    \[
        \sum_{\rho,\nu} \left(\sum_\lambda \zeta_{\lambda\rho} \eta_{\lambda\nu}\right) u_\rho(X) v_\nu(Y) = \sum_\mu u_\mu(X) v_\mu(Y).
    \]
    Since the $u_\lambda(X)v_\lambda(Y)$ are linearly independent as power series, we can compare coefficients, which gives us the desired equality.
\end{proof}

\section{Schur functions}\label{ch:schurs}
\subsection{Combinatorial definitions}
\subsubsection{Schur functions via semistandard Young tableaux}

We recall the definition of a semistandard Young tableau.

\begin{definition}
    A semistandard Young tableau is a Young diagram filled in with entries which increase \textit{weakly} along rows but \textit{strongly} along columns. 
    The set of all semistandard Young tableau of shape $\lambda$ is denoted $\SSYT(\lambda)$.

    The \textit{type} of a semistandard Young tableau $T$, $\type(T)$ is its set of entries counted with multiplicities. Equivalently, it's the tuple $(a_1, a_2, \ldots)$ defined by
    \[
        a_i = (\#\text{of times }i\text{ appears in }T)
    \]
\end{definition}

\begin{example}
    The following Young tableau is semistandard, of shape $4311$ and type $22221$.
    \[
        \begin{ytableau}
            1 & 1 & 2 & 4 \\
            2 & 3 & 3  \\
            4 \\
            5 \\ 
        \end{ytableau}
    \]
\end{example}

\begin{definition}
    Let $T$ be a semistandard Young tableau.  Then define the \textit{weight} of $T$, $x^T$, to be the monomial
    \[
        x^T := x_1^{a_1}x_2^{a_2}\cdots,
    \]
    where $a_1$ is the number of occurrences of $1$ in $T$, $a_2$ is the number of occurrences of $2$ in $T$, and so on.
\end{definition}

\begin{definition}[Schur functions via tableaux]
    Let $\lambda \vdash n$. The \textit{Schur function} $s_\lambda$ is defined to be
    \[
        s_\lambda := \sum_{T \in SSYT(\lambda)} x^T.
    \]
\end{definition}



\begin{example}
    For the partition $22$, we compute $s_\lambda(x_1,x_2,x_3)$.
    We have the following fillings that are semistandard Young tableaux:
    \[
        \ytableausetup{boxsize=normal}
        \begin{ytableau}
            1 & 1 \\
            2 & 2
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 1 \\
            2 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 2 \\
            2 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 1 \\
            3 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            1 & 2 \\
            3 & 3
        \end{ytableau}\:,\:
        \begin{ytableau}
            2 & 2 \\
            3 & 3
        \end{ytableau}\:.
    \]
    These have the weights
    \[
        x_1^2x_2^2, \qquad 
        x_1^2x_2x_3, \qquad 
        x_1x_2^2x_3, \qquad 
        x_1^2x_3^2, \qquad 
        x_1x_2x_3^2, \qquad 
        x_2^2x_3^2
    \]
    respectively. Hence we have computed that
    \[
        s_{22}(x_1,x_2,x_3) = x_1^2x_2^2 + x_1^2x_3^2 + x_2^2x_3^2 + x_1^2x_2x_3 + x_1x_2^2x_3 + x_1x_2x_3^2.
    \]
\end{example}

\subsubsection{Skew Schur functions}
\begin{definition}
    Let $\lambda, \nu$ be partitions such that $\nu \subseteq \lambda$. Then the pair $(\lambda, \nu)$ is referred to as a \textit{skew shape} and is denoted $\lambda \setminus \nu$.

    The \textit{skew diagram} of $\lambda \setminus \nu$ is the diagram obtained by taking $\lambda$'s Young diagram and removing all boxes that would be contained in $\nu$'s Young diagram.

    Finally, a \textit{skew tableau of shape $\lambda \setminus \nu$} or a \textit{tableau of skew shape $\lambda \setminus \nu$} is a filling of the skew diagram of $\lambda \setminus \nu$. 

    Such a tableau will still be called \textit{semistandard} if it weakly increases along rows and strongly increases along columns.
\end{definition}

\begin{example}
    \ytabsmallbox
    Let $\lambda = \ydiagram{4,4,3,2}$ and $\nu = \ydiagram{3,2,2}$. Then $\lambda \setminus \nu$ is

    \ytabnormalbox
    \[\ydiagram{3+1,2+2,2+1,2}.\]
\end{example}

\subsubsection{Kostka numbers}

\begin{definition}
    Let $\lambda \vdash n$ and let $\alpha$ be a weak composition of $n$. The \textit{Kostka number } $K_{\lambda\alpha}$ counts the number of SSYT with shape $\lambda$ and type $\alpha$.

    Let $\nu \subseteq \lambda$. We define the skew Kostka number $K_{\lambda\setminus\nu,\alpha}$ similarly.
\end{definition}

\begin{remark}
    We have that
    \[
        s_\lambda = \sum_\alpha K_{\lambda\alpha}x^\alpha.
    \]
\end{remark}

\begin{theorem} The skew Schurs, and therefore also the Schurs, are symmetric functions.
\end{theorem}

\begin{proof}
    We will prove that $s_{\lambda\setminus\nu}$ is invariant under the action of simple transpositions. 
    Consider the simple transposition $s_i$ which swaps $i$ and $i+1$. 
    Let $\alpha$ be some weak composition of $n$, and define
    \[
        \tilde{\alpha} = s_i\alpha = (\alpha_1, \ldots, \alpha_{i+1}, \alpha_i, \ldots).
    \]
    We wish to now show that 
    \[
        K_{\lambda\setminus\nu,\alpha} = K_{\lambda\setminus\nu,\tilde{\alpha}}.
    \]
    We will prove this by bijection.

    The key fact here is that \textit{if $i$ and $i+1$ appear in the same column, they must be vertically adjacent}. 
    This is due to the column-strictness of semistandard Young tableau.

    Let $T\in\SSYT(\lambda \setminus \nu)$. 
    Take all the columns of $T$ which contain $i$ or $i+1$ but not both. 
    These will form rows consisting of consecutive $i$'s followed by consecutive $i+1$'s. 
    In these rows, swap the number of consecutive $i$'s with the number of consecutive $i+1$'s. 
    This swap works because the $i$'s that become $i+1$'s will not break column strictness since there is no $i+1$ in the same column, similarly $i+1$'s that become $i$'s will not break column strictness since there is no $i$ in the same column.

    This correspondence is involutive and therefore bijective. This completes the proof.
\end{proof}

    For example, if $T$ looked like
    % I desperately need to refactor this
    \[
        \ytabbigbox
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & i & i &  \\
            \none & \none & \none & \none & \none & \none & \none & i & i+1 & i+1\\
            \none & \none & \none & i & i+1 & i+1 & i+1 & i+1 & & \\
            \none & \none & \none & i+1 &   \\
            i & i & i+1
        \end{ytableau},
    \]
    then we ignore all other entries with both $i$ and $i+1$.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red)i & i+1 & i+1 & i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            i & i & i+1
        \end{ytableau}
    \]
    and consider the remaining ones. Then, we keep track of the rows of consecutive $i$ and $i+1$'s.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(yellow) i+1 & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(yellow) i+1 & *(yellow) i+1 & *(yellow) i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(green) i & *(yellow) i+1
        \end{ytableau}
    \]
    Then, we flip the number of consecutive $i$ and $i+1$'s for each row.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(green) i & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(green) i & *(green) i & *(green) i & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(yellow) i+1 & *(yellow) i+1
        \end{ytableau}
    \]

As a consequence,
\begin{corollary}
    Let $\lambda\setminus\nu$ be a skew shape. Then
    \[
        s_{\lambda\setminus\nu} = \sum_{\gamma}K_{\lambda\setminus\nu,\gamma}m_\gamma.
    \]
\end{corollary}

Next, we will prove that the Schurs $s_\lambda$ form a $\ZZ$-basis for $\Lambda$.

\begin{theorem}
    Let $\lambda, \nu \vdash n$. Then $K_{\lambda\nu} \neq 0$ implies $\lambda \geq \nu$. Also, $K_{\lambda\lambda} = 1$.
\end{theorem}

\begin{proof}
    Let $K_{\lambda\nu} \neq 0 $. Then we have the existence of a SSYT $T$ of shape $\lambda$ and type $\nu$. 

    We will show that the entries $1,\ldots,k$ can only appear in the first $k$ rows of $T$.
    Since there can only be $\lambda_1 + \cdots + \lambda_k$ entries in the first $k$ rows of $T$, this automatically tells us that
    \[
        \nu_1 + \cdots + \nu_k \leq \lambda_1 + \cdots + \lambda_k.
    \]

    Suppose that $k$ appeared in the row $i > k$, and let $T_{ij} = k$. 
    Then 
    \[
        1 \leq T_{1j} < T_{2j} < \cdots < T_{kj} < \cdots < T_{ij} = k.
    \]
    But that implies
    \[
        k \leq T_{kj} < \cdots < T_{ij} = k.
    \]
    which gives us $k < k$, a contradiction.

    If $\nu = \lambda$, the only possible SSYT has the $i$th row filled with $i$.
\end{proof}

\begin{corollary}
    $\{ s_\lambda : \lambda \in \Par(n) \}$ forms a basis for $\Lambda^n$, and therefore $\{ s_\lambda : \lambda \in \Par \}$ form a basis for $\Lambda$, as the transition matrix $(K_{\lambda\nu})_{n \prec \lambda, \nu \prec 1^n}$ is lower triangular.
\end{corollary}


\subsection{The Jacobi-Trudi identity}

\subsubsection{Statement}

\begin{theorem} Let $\lambda$ be a partition. Then
    \[
        s_\lambda = \det(h_{\lambda_i+j-i})_{i,j=1}^{\len(\lambda)}.
    \]
\end{theorem}


\subsubsection{The Lindstr\"om-Gessel-Viennot lemma}

\begin{definition} A \textit{digraph} $D$ is a pair consisting of a vertex set $V(D)$ and an arc set $A(D)$ which consists of ordered pairs of vertices. We will suppress the $D$ and refer to the vertex and arc sets as $V$ and $A$.
\end{definition}

\begin{definition}
    A \textit{path} $p$ in a digraph $D$ is an ordered list of arcs $(a_1, \ldots, a_k)$ which are connected end-to-end.
\end{definition}

\begin{definition}
    A \textit{cycle} is a path which starts and ends at the same vertex.
\end{definition}

\begin{definition}
    Let $D$ be a digraph.
    \begin{itemize}
        \item We say that $D$ is acyclic if it contains no cycles.
        \item We say that $D$ is path-finite whenever there exist only finitely many paths from $u$ to $v$ for all $u,v \in V$.
        \item Let $\KK$ be a ring. We say that $D$ is \textit{weighted} when we have a function $w: A \to \KK$ that assigns a \textit{weight} to each arc of $D$.
    \end{itemize}
\end{definition}

\begin{theorem}[Lindstr\"om-Gessel-Viennot] 
    Let $D$ be an weighted, acyclic path-finite, digraph.

    Let $U, V$ be two sets of $n$ vertices in $D$. Define the \textit{weight} of a path $p$ to be 
    \[
        w(p) = \prod_{a \in p} a
    \]
    For any two vertices $u, v$ of $D$, define the quantity $\phi(u, v)$ to be
    \[
        \phi(u,v) = \sum_{p:u \rightarrow v} w(p),
    \]
    where $p: u \rightarrow v$ means that $p$ is a path from $u$ to $v$.

    Consider now the determinant 

    \[
        \det \begin{bmatrix}
            \phi(u_1, v_1) & \cdots & \phi(u_n, v_1) \\
             \vdots & \ddots & \vdots \\
            \phi(u_1, v_n) & \cdots & \phi(u_n, v_n)
        \end{bmatrix}
    \]

    \todo{Finish proof of LGV}
\end{theorem}

\subsubsection{Proof}

\begin{proof}[Proof of the Jacobi-Trudi identity]
    Fix $N \in \NN$. Consider the digraph $D$ whose vertex set is $\NN \times \{1,\ldots,N\}$. 

    We assign weights to the edges so that all vertical arcs are weighted $1$, and all horizontal arcs $(i, j) \to (i+1, j)$ are weighted $x_{j+1}$.

    For example, consider the following path
    \begin{center}
        \begin{tikzpicture}[
            scale=2,
            font=\Large
            ]
            \draw[
                step=1cm,
                line width = 0.05cm,
                ] (0,0) grid (3,3);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (0,0) -- node[above] {$x_1$} ++(1,0);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (1,0) -- ++(0,1);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (1,1) -- ++(0,1);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (1,2) -- node[above] {$x_3$} ++(1,0);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (2,2) -- node[above] {$x_3$} ++(1,0);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (3,2) --++ (0,0.75);
            \filldraw[
                fill=white,
                draw=black,
                line width=0.05cm,
                radius=0.25cm,
                ] (0,0) circle node {$u$};
            \filldraw[
                fill=white,
                draw=black,
                line width=0.05cm,
                radius=0.25cm
                ] (3,3) circle node {$v$};
        \end{tikzpicture}
    \end{center}

    This path's weight is $x_1x_3^2$.

    Paths $p: (i,1) \to (i+n, N)$ are in bijection with monomials in $\KK[X_N]$ of degree $n$.



    We lay out the parts of $\lambda$ in ascending order at $x=1$.

    \todo{Finish proof of Jacobi-Trudi}
\end{proof}

\section{The Robinson-Schensted-Knuth correspondence}

I'm going to define RSK functionally instead of imperatively. 

(After the fact, I think this was a waste of time and was more or less a cacophony of notation, but it was interesting regardless.)

\subsection{Row insertion}

\todo{Rewrite row insertion}

The basic operation will be that of \textit{row insertion}.

\begin{definition}[Row insertion]
    Let $P = (P_{ij})$ be a SSYT of shape $\lambda$. And let $t \in \NN$. We will define the \textit{insertion path} $I(P\leftarrow t)$ now.
    We write that $t \geq P_m$ whenever $t \geq \max P_m$ or $P_m$ is empty and $t < P_m$ otherwise. 
    If $t < P_m$, define
    \begin{align*}
        (P_m \leftarrow t) := \begin{cases}
            \lambda_m + 1 & \text{if $t \geq \max P_m$ or $P_m$ is empty} \\
            \min\{n:P_{mn} > t\} & \text{otherwise}.
        \end{cases}.
    \end{align*}
    We note that $(P_m \leftarrow t) \leq \lambda_m + 1$ always.
    We will define a recursive function $\RowInsert$ as follows.
    \begin{align*}
        &\RowInsert(P,m,t) \\
        &:= \begin{cases}
            [(P_m \leftarrow t)] & t \geq P_m \\
            [(P_m \leftarrow t)] + \RowInsert(P, m+1, P_{m(P_m \leftarrow t)}) & t < P_m
        \end{cases}.
    \end{align*}
    Where $+$ denotes list concatenation.
    Now, define $I(P\leftarrow t) := \RowInsert(P,1,t).$
\end{definition}

Now we define the tableau that results from row insertion, $(P \leftarrow t)$.

\begin{definition}
    Let $P$ be a SSYT and let $t \in \NN$. Let $I(P \leftarrow t) = [j_1, j_2, \ldots, j_M]$.
    \[
        (P \leftarrow t)_{rs} := \begin{cases}
            t & \text{if } (r,s) = (1, j_1) \\
            P_{m-1,j_{m-1}} & \text{if }(r,s) = (m, j_m) \\
            P_{rs} & \text{otherwise}.
        \end{cases}.
    \]
\end{definition}

We state a property of insertion paths that will allow us to prove that row insertion gives us SSYT.

\begin{theorem}
    Let $P$ be a SSYT of shape $\lambda$, let $t \in \PP$, and let $I(P\leftarrow t) = [j_1,j_2,\ldots,j_M]$. Then $I(P\leftarrow t)$ is weakly decreasing.
\end{theorem}

\begin{proof}
    Let $1 \leq m < M$.
    Suppose that $\lambda_{m+1} < j_m$. Then 
    \[
        j_{m+1} = (P_m \leftarrow k) \leq \lambda_m+1 \leq j_m.
    \]
    Now suppose that $\lambda_{m+1} \geq j_m$. 
    Then $P_{m+1,j_m}$ exists, and it must be that $P_{mj_m} < P_{m+1,j_m}$ due to column strictness. Then
    \[
        j_{m+1} = (P_{m+1} \leftarrow P_{mj_m}) = \min \{n:P_{m+1,n}>P_{mj_m}\} \leq j_m.
    \]
    Either way $j_{m+1} \leq j_m$.
\end{proof}

\begin{corollary}\label{InsertionPreservesSSYT}
    If $P$ is a SSYT and $t \in \PP$, then $(P \leftarrow t)$ is a SSYT.
\end{corollary}

\begin{proof}
    By the definition of row insertion, the rows are weakly increasing. Consider inserting a bumped number. By the previous theorem, it can only be moved down or down left, which means that it will always be inserted below a smaller number. This continues for the whole insertion path.
\end{proof}

\subsection{Generalized two-line notation}

\begin{definition}
    Let $A = (a_{ij})$ be a $\NN$-matrix. Let $i, j \in \NN$, and write a \textit{two-line entry} to be
    \[
        \binom{i}{j}.
    \]
    We concatenate two-line entries as follows
    \[
        \binom{i}{j} + \binom{k}{l} := \binom{i\:k}{j\:l}.
    \]
    Now consider the following formal sum:
    \begin{align*}
        w_A &= \sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij}\binom{i}{j} \\
            &= \sum_{i=1}^\infty \left(a_{i1}\binom{i}{1} + a_{i2}\binom{i}{2} + \cdots\right) \\
            &= \left(a_{11}\binom{1}{1} + a_{12}\binom{1}{2} + \cdots\right) + \left(a_{21}\binom{2}{1} + a_{22}\binom{2}{2} + \cdots\right) + \cdots.
    \end{align*}
    If $A$ has finite support, $w_A$ is well-defined. 
    $w_A$ is the \textit{two-line array} or \textit{generalized permutation} corresponding to the matrix $A$. We note that in $w_A$ the individual two-line entries are sorted in lexicographic order.
\end{definition}

\begin{example}
    Let $A$ be the matrix
    \[
        \begin{bmatrix}
            1 & 0 & 3 & 2 \\
            0 & 1 & 0 & 0 \\
            2 & 4 & 1 & 0 \\
            0 & 0 & 1 & 0
        \end{bmatrix}.
    \]
    Then 
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 & 1 & 1 & 1 \
              & 2 \
              & 3 & 3 & 3 & 3 & 3 & 3 & 3\
              & 4 \\
            1 \
              & 2 & 2 & 2 \
              & 4 & 4 \
              & 2 \
              & 1 & 1 \
              & 2 & 2 & 2 & 2 \
              & 3 \
              & 3
        \end{pmatrix}
    \]
\end{example}

\subsection{The RSK algorithm}

\begin{definition}
    We define the \textit{RSK correspondence} to be a rule that assigns to $\NN$-matrices of finite support to pairs of SSYT $(P, Q)$, which we notate
    \[
        A \rskarrow (P, Q).
    \]
    To compute $(P, Q)$, let 
    \[
        w_A = \begin{pmatrix}
            i_1 & i_2 & \cdots & i_m \\
            j_1 & j_2 & \cdots & j_m
        \end{pmatrix},
    \]
    and begin with $(P_0, Q_0) = (\varnothing, \varnothing)$. Iteratively construct $(P_{t+1}, Q_{t+1})$ from $(P_t,Q_t)$ as follows
    \begin{enumerate}[label=(\alph*)]
        \item $P_{t+1} = P_t \leftarrow j_{t+1}$
        \item Add a new box to $Q_t$ where a new box to $P_t$ was added, and fill it with $i_{t+1}$.
    \end{enumerate}
    This process continues until $t=m$, and we take $(P, Q) = (P_m, Q_m)$.

    $P$ is referred to as the \textit{insertion tableau} and $Q$ is referred to as the \textit{recording tableau}.
\end{definition}



\begin{example}
    Let 
    \[
        A := \begin{bmatrix}
            1 & 0 & 2 \\
            0 & 2 & 0 \\
            1 & 1 & 0
        \end{bmatrix}.
    \]
    We have that
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 \
              & 2 & 2 \
              & 3 & 3 \\
            1 \
              & 3 & 3 \
              & 2 & 2 \
              & 1 & 2
        \end{pmatrix}.
    \]
    So, going through the insertion process,

    \ytableausetup{boxsize=10pt}
    \begin{center}
        \ytabmathmode
        \begin{tabular}{r | l | l}
            $t$ &  $P_t$ & $Q_t$ \\
            \hline 
            $1$ \
              & \begin{ytableau} 
                  \none
                \end{ytableau} \
              & \begin{ytableau} 
                  \none
              \end{ytableau} \\
            $2$ \
              & \begin{ytableau} 
                    *(green) 1
                \end{ytableau} \
              & \begin{ytableau} 
                  *(green) 1
              \end{ytableau} \\
            $3$ \
              & \begin{ytableau} 
                  1 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & *(green) 1
              \end{ytableau} \\
            $4$ \
              & \begin{ytableau} 
                  1 & 3 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & *(green) 1
              \end{ytableau} \\
            $5$ \
              & \begin{ytableau} 
                  1 & *(yellow) 2 & 3 \\
                  *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1 \\
                  *(green) 2
              \end{ytableau} \\
            $6$ \
              & \begin{ytableau} 
                  1 & 2 & *(yellow) 2 \\
                  3 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1  \\
                  2 & *(green) 2
              \end{ytableau} \\
            $7$ \
              & \begin{ytableau} 
                  1 & *(yellow) 1 & 2 \\
                  *(yellow) 2 & 3 \\
                  *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1  \\
                  2 & 2 \\
                  *(green) 3
              \end{ytableau} \\
            $8$ \
              & \begin{ytableau} 
                  1 & 1 & 2 & *(green) 2 \\
                  2 & 3 \\
                  3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1 & *(green) 3 \\
                  2 & 2 \\
                  3
              \end{ytableau} \\
        \end{tabular} 
    \end{center}  

    In the above table, green squares are inserted, whereas yellow squares are bumped.
    We find that
    \[
        P = \ytableaushort{1122,23,3} \qquad Q = \ytableaushort{1113,22,3}.
    \]
\end{example}

\begin{theorem}
    RSK is a bijection between $\NN$-matrices $A$ of finite support and pairs of SSYT $(P,Q)$ such that
    \begin{align*}
        \type(P) &= \col(A), \\
        \type(Q) &= \row(A).
    \end{align*}
\end{theorem}

\begin{proof}
    That the types of $P$ and $Q$ correspond to row and column sums of $A$ is obvious-- $j$ coordinates with multiplicities get inserted into $P$, while $i$ coordinates with multiplicities get inserted into $Q$.
    That $P$ is a SSYT follows from (\ref{InsertionPreservesSSYT}).
    That $Q$ is a SSYT follows from properties of the insertion path.

    Now, it remains to prove that the RSK correspondence is a bijection. 
    RSK can actually be run backwards. 
    First, we use this to prove injectivity, by showing that running RSK backwards is actually the inverse of RSK. 
    Second, we use this to prove surjectivity, by showing that backwards RSK works for arbitrary pairs of SSYT.

\end{proof}

\subsection{Implications}


\begin{theorem}[Cauchy identity]    We have
    \begin{equation}\label{eq:CauchyIdentityForSchurs}
        \prod_{i,j \geq 1}(1-x_iy_j)^{-1} = \sum_{\lambda \in \Par}s_\lambda(X) s_\lambda(Y).     
    \end{equation}
\end{theorem}

\begin{proof}
    We have that
    \begin{align*}
        [x^\alpha y^\beta]\left(\prod_{i,j\geq1}(1-x_iy_j)^{-1}\right) &= \left(\substack{\#\text{ of $\NN$-matrices $(a_ij)$ such that}\\\row(a) = \alpha\text{, and }\col(a) = \beta}\right) \\
        [x^\alpha y^\beta]\left(\sum_{\lambda\in\Par} s_\lambda(X) s_\lambda(Y) \right) &= \left(\substack{\#\text{ of pairs $(P,Q)$ of SSYT of the same shape such that}\\\type(P) = \alpha\text{, and }\type(Q) = \beta}\right).
    \end{align*}
    RSK tells us that both counts are the same, hence both sides of the identity are equal as power series.
\end{proof}

\begin{corollary}
    The Schurs are an orthonormal basis of $\Lambda$ with respect to the Hall inner product.
\end{corollary}
\begin{proof}
    This follows from Theorem \ref{thm:CauchyIdentityImpliesOrthonormal}.
\end{proof}

\begin{corollary}
    Let $\mu,\nu \vdash n$. Then
    \[
        \sum_{\lambda \vdash n} K_{\lambda\mu}K_{\lambda\nu} = \langle h_\mu, h_\nu \rangle.
    \]
\end{corollary}

\begin{proof}
    Take the coefficient of $x^\mu y^\nu$ in (\ref{eq:CauchyIdentityForSchurs}).
\end{proof}

\begin{corollary}\label{thm:HToSIsKostka}
    We have that
    \[
        h_\mu = \sum_\lambda K_{\lambda\mu}s_\lambda.
    \]
    Equivalently,
    \[
        \langle h_\mu, s_\lambda \rangle = K_{\lambda\mu}.
    \]
\end{corollary}

\cite{StanleyEC2} gives three proofs of this corollary. The first one is ``the fastest''.

\begin{proof}[Proof via the Hall inner product]
    We know that
    \[
        s_\lambda = \sum_\nu K_{\lambda\nu} m_\nu,
    \]
    so 
    \begin{align*}
        \langle h_\mu, s_\lambda \rangle &= \left\langle h_\mu, \sum_\nu K_{\lambda\nu} m_\nu \right\rangle \\
                                         &=\sum_{\nu} K_{\lambda\nu} \langle h_\mu, m_\nu \rangle \\
                                         &=\sum_{\nu} K_{\lambda\nu} \delta_{\mu\nu} \\
                                         &= K_{\lambda\mu}.
    \end{align*}
\end{proof}

This next one is really the same thing, but packaged differently.

\begin{proof}[Proof via Cauchy Identities]
    We have that
    \[
        \sum_\lambda m_\lambda(X)h_\lambda(Y) = \sum_\lambda s_\lambda(X) s_\lambda(Y),
    \]
    since both equal $\prod_{i,j}(1-x_iy_j)^{-1}$.
    So we have that
    \begin{align*}
        \sum_\mu m_\mu(X)h_\mu(Y) &= \sum_\lambda \left( \sum_\mu K_{\lambda\mu} m_\mu(X) \right) s_\lambda(Y) \\
                                  &= \sum_\lambda \sum_\mu m_\mu(X) K_{\lambda\mu} s_\lambda(Y) \\
                                  &= \sum_\mu \sum_\lambda m_\mu(X) K_{\lambda\mu} s_\lambda(Y) \\
                                  &= \sum_\mu m_\mu(X) \left(\sum_\lambda K_{\lambda\mu} s_\lambda(Y)\right)
    \end{align*}
    We already know that the $m_\mu(X)$'s are linearly independent; we finish the proof by equating their coefficients.
\end{proof}

This last proof is purely combinatorial, but is also again really the same thing. 

\begin{proof}[Proof via RSK]
    \begin{align*}
        h_\mu = \sum_{\substack{A:\NN-\text{matrix}\\\row A=\mu}} x^{\col A}
              \qquad\rskarrow\qquad
              &=\sum_{\substack{P,Q:\SSYT^2\\\type P=\mu}} x^Q  \\
              &= \sum_\lambda K_{\lambda\mu} \left(\sum_{\substack{Q:\SSYT\\\shape Q=\lambda}} x^Q\right) \\
              &= \sum_\lambda K_{\lambda\mu} s_\lambda
    \end{align*}
\end{proof}

\begin{corollary}
    We have that
    \[
        h_{1^n} = \sum_{\lambda \vdash n} f^\lambda s_\lambda.
    \]
\end{corollary}

\begin{proof}
    Combine Corollary \ref{thm:HToSIsKostka} and the fact that $f^\lambda = K_{\lambda,1^n}$.
\end{proof}

\subsection{Standardization}
We can \textit{standardize} two-line arrays, which gives us another two-line array with no repeated entries.

Consider a row of numbers $(i_1\: \ldots\: i_n)$. 
We create a tableau that fills each position in the row in a corresponding box, where a box $(s,t)$ corresponds to the $t$-th appearance of $s$ from the left.
For example, the array
\[
    \begin{pmatrix}
        1 & 1 & 1 & 2 & 2 & 3 & 3 & 3 & 3
    \end{pmatrix}
\]
becomes
\[
    \ytabnormalbox
    \ytableaushort{123,45,6789}*{3,2,4}
\]
Then, we fill in all the squares with $1,\ldots,n$ going in rows.
In our example, this is already how the rows are filled!
To get our standardized row, $\tilde{i}_n$ will be the entry of the second tableau in the box where $n$ appears in the first tableau.
So the standardized row from our example is actually
\[
    \begin{pmatrix}
        1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9
    \end{pmatrix}.
\]
It's not hard to see that this \textit{always} happens if our row is weakly increasing.

Consider the row
\[
    \begin{pmatrix}
        1 & 1 & 3 & 2 & 3 & 1 & 2 & 2 & 2
    \end{pmatrix}.
\]
Our first tableau is now
\[
    \ytabnormalbox
    \ytableaushort{126,4789,35}*{3,4,2},
\]
and so our second tableau is
\[
    \ytabnormalbox
    \ytableaushort{123,4567,89}*{3,4,2}
\]
Applying standardization, we get
\[
    \begin{pmatrix}
        1 & 2 & 8 & 4 & 9 & 3 & 5 & 6 & 7
    \end{pmatrix}
\]
We can now define the following:

\begin{definition}
    Let $w_A = \binom{i_1\cdots i_n}{j_1\cdots j_n}$ be a two-line array arising from the $\NN$-matrix $A$.

    We define the \textit{standardized two-line array} $\widetilde{w}_A$ to be $w_A$ with both of its rows standardized. 
\end{definition}

In effect, we will always get 
\[
    \begin{pmatrix}
        1 & \cdots & n \\
        \widetilde{j_1} & \cdots & \widetilde{j_n}
    \end{pmatrix}.
\]
where $\widetilde{j_i}$ is $j_i$ standardized.

\begin{example}
    Combining the first two examples of standardized rows, if
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 & 2 & 2 & 3 & 3 & 3 & 3 \\
            1 & 1 & 3 & 2 & 3 & 1 & 2 & 2 & 2
        \end{pmatrix},
    \]
    then
    \[
        \widetilde{w}_A = \begin{pmatrix}
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
1 & 2 & 8 & 4 & 9 & 3 & 5 & 6 & 7
        \end{pmatrix}.
    \]
\end{example}

\begin{lemma}
    Standardization commutes with RSK.
    Meaning, if you keep track of the map of entries going from $w_A$ to $\widetilde{w}_A$, then reversing this map on $\widetilde{P},\widetilde{Q}$ gives you $P,Q$ respectively.
\end{lemma}

\subsection{Symmetry}

\begin{theorem}\label{thm:rsksymmetry}
    If $A \rskarrow (P,Q),$ then $A^T \rskarrow (Q,P).$
\end{theorem}

The first proof given in \cite{StanleyEC2} makes use of the \textit{inversion poset} of a permutation.

\begin{definition}
    Consider the two-line array
    \[
        w = \begin{pmatrix}
            i_1 & \cdots & i_k \\
            j_1 & \cdots & j_k
        \end{pmatrix}
    \]
    and define the \textit{inversion poset} $I$ to be the poset whose elements are the pairs $(i_t, j_t)$ for $1 \leq t \leq k$, and whose partial order is given by putting $(a,b) \leq (c,d)$ whenever $a \leq b$ and $c \leq d$.
\end{definition}

For convience, we will use the compact notation $i_tj_t$.
For example, the pair $(3,5)$ will just be written $35$.

\begin{example}\label{ex:invposet}
    Let
    \[
        w = \begin{pmatrix}
            1 & 2 & 3 & 4 & 5 & 6 & 7 \\
            5 & 1 & 6 & 7 & 4 & 3 & 2
        \end{pmatrix}.
    \]
    Then the inversion poset of $w$ is 
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
        \end{tikzpicture}.
    \end{center}
\end{example}

Given an inversion poset $I$, we may partition it into antichains $I_1, \ldots, I_k$ as follows:
\begin{enumerate}
    \item Let $I_1$ be the set of minimal elements of $I$.
    \item Suppose that $I_1, \ldots, I_j$ are defined. 
        Then define $I_{j+1}$ to be the minimal elements of $I \setminus (I_1 \cup \cdots \cup I_j)$.
\end{enumerate}

\begin{example}\label{ex:invposetantichains}
    For the same $w$ defined in Example \ref{ex:invposet}, the antichains are
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
            \node[red] (i1) at (-0.5,0) {$I_1$};
            \node[red] (i2) at (-0.5,1) {$I_2$};
            \node[red] (i3) at (-0.5,2) {$I_3$};
            \draw[thick,red,rounded corners] (-0.3, -0.3) rectangle (1.3, 0.3) {};
            \draw[thick,red,rounded corners] (-0.3, 0.7) rectangle (3.3, 1.3) {};
            \draw[thick,red,rounded corners] (-0.3, 2.3) rectangle (0.3, 1.7) {};
        \end{tikzpicture}.
    \end{center}
\end{example}

Moreover, considering the definition of the inversion poset's order, we may define an order on its antichains, given by $(a,b) < (c,d)$ whenever $a < c$ and $b > d$.

This defines a total order on any antichain of $I$ (check it!).

\begin{example}
    Continuing Example \ref{ex:invposetantichains}, the order internal to each antichain (going east to west from least to greatest) is
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
            \node[red] (i1) at (-0.5,0) {$I_1$};
            \node[red] (i2) at (-0.5,1) {$I_2$};
            \node[red] (i3) at (-0.5,2) {$I_3$};
            \draw[thick,red,rounded corners] (-0.3, -0.3) rectangle (1.3, 0.3) {};
            \draw[thick,red,rounded corners] (-0.3, 0.7) rectangle (3.3, 1.3) {};
            \draw[thick,red,rounded corners] (-0.3, 2.3) rectangle (0.3, 1.7) {};
            \draw[red] (15) -- (21);
            \draw[red] (36) -- (54) -- (63) -- (72);
        \end{tikzpicture},
    \end{center}
    where the covering relation is notated with red edges.
\end{example}

We can now state an important lemma that makes use of all the machinery we've just built up.

\begin{lemma}
    Consider a two-line word $w$, and let $w \rskarrow (P, Q)$.
    Suppose we've constructed $I$ and $I_1, \ldots I_k$ given $w$.
    Then, the first row of $P$ has $k$ entries, and is given by
    \[
        P_{1,r} = \text{the top number of }\min I_r,
    \]
    and the first row of $Q$ also has $k$ entries and is given by
    \[
        Q_{1,r} = \text{the bottom number of }\max I_r.
    \]
\end{lemma}

\begin{example}
    Completing the example, we compute the top and bottom numbers of the least and greatest elements of each antichain.
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
            \begin{scope}[thick,red,rounded corners]
                \draw (-0.3, -0.3) rectangle (1.3, 0.3) {};
                \draw (-0.3, 0.7) rectangle (3.3, 1.3) {};
                \draw (-0.3, 2.3) rectangle (0.3, 1.7) {};
            \end{scope}
            \begin{scope}[color=red]
                \draw (15) -- (21);
                \draw (36) -- (54) -- (63) -- (72);
            \end{scope}
            \begin{scope}[color=blue,-{Latex[length=3mm]}]
                \draw (-0.5,2) -- (-1.5,2);
                \draw (-0.5,1) -- (-1.5,1);
                \draw (-0.5,0) -- (-1.5,0);
            \end{scope}
            \begin{scope}[color=green,-{Latex[length=3mm]}]
                \draw (0.5,2) -- (4.5,2);
                \draw (3.5,1) -- (4.5,1);
                \draw (1.5,0) -- (4.5,0);
            \end{scope}
            \begin{scope}
                \node at (-2, 2) {$4$};
                \node at (-2, 1) {$3$};
                \node at (-2, 0) {$1$};
            \end{scope}
            \begin{scope}
                \node at (5, 2) {$7$};
                \node at (5, 1) {$2$};
                \node at (5, 0) {$1$};
            \end{scope}
        \end{tikzpicture}
    \end{center}
    So the first row of $P$ is $\ytableaushort{134}$ and the first row of $Q$ is $\ytableaushort{127}$.
\end{example}

\todo{Finish proofs of RSK symmetry}
\begin{proof}[Proof of Theorem \ref{thm:rsksymmetry} using the inversion poset]

\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:rsksymmetry} using Fomin growth diagrams]
\end{proof}

\section{Solutions to Exercises}

\begin{exercise}
    $[1]$
    True or false: The Ferrers diagram of the partition $(4,4,3)$ is given by
    \[
        \begin{matrix}
            . & . & . & . \\
            . & . & . & . \\
            . & . & . & .
        \end{matrix}
    \]
\end{exercise}

True! The bottom-right-most dot is in fact the sentence's period. (I hate this book!)

\begin{exercise}
    Let $\Par(n)$ denote the set of all partitions of $n$ with the dominance ordering.
    \begin{enumerate}[label=(\alph*)]
        \item $[2]$ Show that $\Par(n)$ is a lattice.
        \item $[2+]$ Show that $\Par(n)$ is self-dual.
        \item $[2+]$ Find the smallest value of $n$ for which $\Par(n)$ is not graded.
        \item $[2+]$ Show that the maximum number of elements covered by an element of $\Par(n)$ is $\lfloor \frac{1}{2}(\sqrt{1+8n}-3)\rfloor$.
        \item $[2+]$ Show that the shortest maximal chain in $\Par(n)$ has length $2n-4$ for $n \geq 3$.
        \item $[3-]$ Show that the longest maximal chain in $\Par(n)$ has length
            \[
                \frac{1}{3}m(m^2+3r-1) \sim \frac{1}{3}(2n)^{3/2},
            \]
            where $n = \binom{m+1}{2} + 2,\:0 \leq r \leq m$.
    \end{enumerate}
\end{exercise}

Consider dividing a partition's diagram into rectangles of parts of the same size. For example, if $\lambda = \ydiagram{4,3,3,1}$, we divide it into the three rectangles $r_1 = 4^1, r_2 = 3^2, r_3 = 1^1$.
\[
    \ytabnormalbox
    \ytableaushort[r_]{1111,222,222,3}
\]
Given a partition $\lambda$ with rectangles $(r_1, \ldots)$, we define a a new partition from a \textit{tap} of rectangle $r_i$, where $i > 1$, to be taking the bottom-right-most box of $r_i$ and adding it to the first row of $r_{i-1}$.

For example, if we tapped $r_2$ in the above diagram, we get
\[
    \ytableaushort{}*{4,3,3,1}*[*(green)]{4}*[*(yellow)]{0,3,3}
    \xrightarrow{\text{tap second rectangle}} 
    \ytableaushort{}*{0,0,0,1}*[*(green)]{4}*[*(yellow)]{4+1,3,2}
\]
Evidently, this breaks the rectangular division of $\lambda$, so we obtain new rectangles.

\begin{lemma}
    Suppose $\lambda$ and $\mu$ agree on the first $t$ rows. Then they both agree with $\lambda \wedge \mu$ on the first $t$ rows. Moreover, let $\lambda'$ and $\mu'$ be the respective partitions with the first $t$ rows removed. Then $\lambda \wedge \mu$ is $\lambda' \wedge \mu'$ prepended with the $t$ removed rows.
\end{lemma}
\begin{proof}
    Suppose $\lambda_1 = \mu_1, \ldots, \lambda_t = \mu_t$.
    Let $\nu \geq \lambda$ and $\nu \geq \mu$.
    Suppose $\nu$ disagrees with $\mu$ and $\lambda$ at $s$-th row, where $s < t$.
    It must be that $\nu_s > \lambda_s$, since $\nu \geq \lambda$. 
\end{proof}

Now let $\lambda, \mu$ be two partitions of $n$. 
We give an explicit construction for $\nu := \lambda \wedge \mu$, which we construct by stabilizing one row at a time.

Put $\lambda^{(0)} := \lambda$ and $\mu^{(0)} := \mu$. We will construct two sequences of partitions $(\lambda^{(j)})$ and $(\mu^{(j)})$ such that $\nu_i = \lambda^{(i)}_i = \mu^{(i)}_i$.


Suppose $\lambda^{(0)}, \ldots, \lambda^{(j-1)}$, $\mu^{(0)}$, \ldots, $\mu^{(j-1)}$ have been constructed. 
To construct $\lambda^{(j)}, \mu^{(j)}$, we consider three cases
\begin{itemize}
    \item Case $1$: $\lambda_j = \mu_j$. We put 
        \begin{align*}
            \nu_j &= \nu_{j-1}, \\
            \lambda^{(j)} &= \lambda^{(j-1)}, \\
            \mu^{(j)} &= \mu^{(j-1)}.
        \end{align*}
    \item Case $2$: $\lambda_j > \mu_j$. We put 
        \begin{align*}
            \nu_j &= \lambda_j, \\
            \lambda^{(j)} &= \lambda^{(j-1)}, \\
            \mu^{(j)} &= \overline{\mu^{(j-1)}},
        \end{align*}
        where $\overline{\mu^{(j-1)}}$ is the tableau obtained by tapping the the nearest rectangle to the $j$-th row of $\mu$ until the $j$-th row has $\lambda_j$ squares.
    \item Case $3$: $\lambda_j < \mu_j$. We put 
        \begin{align*}
            \nu_j &= \mu_j, \\
            \lambda^{(j)} &= \overline{\lambda^{(j-1)}}, \\
            \mu^{(j)} &= \mu^{(j-1)},
        \end{align*}
        where $\overline{\lambda^{(j-1)}}$ is defined similarly as above.
\end{itemize}

We claim that, not only are the above sequences, and therefore $\nu$, are well defined, we also have that $\nu = \lambda \wedge \mu$ as desired.

We now have a smaller tableau of size $n - |\mu_1 - \lambda_1|$.


\begin{exercise}
    $[2+]$
    Expand the power series $\prod_{i\geq1}(1+x_i+x_i^2)$ in terms of the elementary symmetric functions.
\end{exercise}

Let $f$ be the above power series. 
Picking a term of $f$ amounts to choosing some composition $\alpha$ with parts $0,1,2$. Hence
\[
    f = \sum_{p,q \geq 0} m_{1^p,2^q}
\]
We 

\begin{exercise}
    $[2+]$
    Show that
    \[
        h_r(x_1,\ldots,x_n) = \sum_{k=1}^n x_k^{n-1+r}\prod_{i\neq k}(x_k-x_i)^{-1}.
    \]
\end{exercise}

\begin{exercise}
    $[2+]$
    \begin{enumerate}[label=(\alph*)]
        \item $[2+]$ Prove the identity
            \begin{equation}\label{eq:124}
                \left(1-\sum_{n\geq1}p_nt^n\right)^{-1} = \frac{\sum_{n\geq0}h_nt^n}{1-\sum_{n\geq1}(n-1)h_nt^n}
            \end{equation}
        \item $[2+]$ Let $Y_n$ denote the coefficient of $t^n$ in the expansion of either side of equation (\ref{eq:124}), so $Y_n \in \Lambda^n$. Show that
            \[
                Y_n = \sum_w p_{\rho(w)},
            \]
            where $w$ ranges over all permutations in $\frkS_n$ satisfying $w(i) \geq i-1$ for all $i$.
    \end{enumerate}
\end{exercise}

\begin{exercise}
    $[2+]$
    Let $w \in \frkS_n$ have cycle type $\lambda$. Give a direct bijective proof of 
\end{exercise}


\begin{thebibliography}{999999}
    \raggedright\footnotesize

    \bibitem[StanleyEC2]{StanleyEC2}
    Richard P. Stanley, \textit{Enumerative Combinatorics. Volume 2}, Cambridge University Press 2023.

    \bibitem[GrinbergAC]{DarijAC}
    Darij Grinberg, \textit{An Introduction to Algebraic Combinatorics}, \url{http://www.cip.ifi.lmu.de/~grinberg/t/21s/lecs.pdf}

    \bibitem[GrinbergSGA]{DarijSGA}
    Darij Grinberg, \textit{An introduction to the symmetric group algebra}, \url{http://www.cip.ifi.lmu.de/~grinberg/t/24s/sga.pdf}

    \bibitem[WildonSymFuncs]{WildonSymFuncs}
    Mark Wildon, \textit{An Involutive Introduction to Symmetric Functions}, \url{http://www.ma.rhul.ac.uk/~uvah099/Maths/Sym/SymFuncs2020.pdf}
\end{thebibliography}

\end{document}
