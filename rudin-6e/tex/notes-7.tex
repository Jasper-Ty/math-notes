\section{Pointwise Convergence}

\begin{result}
    {Pointwise convergence of functions}
    {Definition}

    Let $f_n: E \rightarrow \mathbb{R}$ be a sequence of functions. If $f$ is a function such that $f_n(x) \to f(x)$ as $n \to \infty$ for all $x \in E$, then we say $f_n$ converges \textit{pointwise} to $f$.
\end{result}

This type of convergence is very \textit{weak}. It guarantees very little in the way of actually \textit{working with} the limit.

This definition is readily adapted to infinite sums of functions.

\begin{result}
    {Infinite sums of functions}
    {Definition}
    
    If $f$ is a function such that 
    \[\sum_{n=1}^\infty f_n(x) = f(x)\] 
    for all $x \in E$, then we say $f$ is the \textit{sum} of the series $f_n$.
\end{result}

An example of the weakness of pointwise convergence is

\begin{result}
    {Continuity is not preserved under pointwise convergence}
    {Example} 

    Let $f_n: [0, 1] \rightarrow [0, 1]$ be defined by
    \[f_n(x) := x^n\]

    Then $f := \lim f_n$ is 
    \[f(x) = \begin{cases}
        0 & x < 1 \\
        1 & x = 1
    \end{cases}\]

    by Theorem 3.20(e)
\end{result}

In this case, a sequence of continuous functions converges to a function that is eminently discontinuous. We use the preceding idea of ``letting $f < 0$ sink and letting $f = 1$ float using the $n^{th}$ power limit'' to show the following.

\begin{result}
    {Integrability is not preserved under pointwise convergence}
    {Example}

    \[\lim_{m \to \infty} \lim_{n \to \infty} (\cos m!\pi x ) ^{2n} = \begin{cases}
        0 & x\text{ irrational} \\
        1 & x\text{ rational}
    \end{cases}\]

    If we let 
    \[f_m (x) := \lim_{n \to \infty} (\cos m!\pi x)^{2n}\]
    the above shows that a limit of integrable functions ($\int f_m dx = 0$ for all $m$) may fail to be integrable.

    \textbf{Proof}

    By a similar argument as in the previous example,
    \[\lim_{n \to \infty} (\cos m!x)^{2n} = \begin{cases}
        0 & m!x\text{ is not an integer} \\
        1 & m!x\text{ is an integer} \\
    \end{cases}\]

    Let $x = p/q$ be rational. Then $m!x$ is rational for all $m \geq q$. Let $x$ be irrational, $m!x$ cannot be an integer for any $m$, otherwise we can show a contradiction. Then

    \[\lim_{m \to \infty} \begin{cases}
        0 & m!x\text{ is not an integer} \\
        1 & m!x\text{ is an integer} \\
        \end{cases} = \begin{cases}
        0 & x \text{ irrational} \\
        1 & x \text{ rational} \\
    \end{cases}\]
\end{result} 

These two examples show that \textit{properties} of $f_n$ may not pass through the limit to $f$. 

Next, we show that \textit{operations} on $f_n$ may not be passed through the limit to $f$.

\begin{result}
    {A limit of differentiated functions may not be the differentiated limit of functions}
    {Example}

    Let 
    \[f_n(x) := \frac{\sin nx}{\sqrt{n}}\]
    Then,
    \[0 = \frac{d}{dx}\left[\lim_{n \to \infty} f_n \right]\neq \lim_{n \to \infty} \left[\frac{d}{dx} f_n\right] = \sqrt{n}\cos nx\]
\end{result}

\begin{result}
    {A limit of integrated functions may not be the integral of a limit of functions}
    {Example}

    Let 
    \[f_n(x) := n x(1-x^2)^n\]
    Then
    \[0 = \int_0^1 \left[\lim_{n \to \infty} f_n \right] \neq \lim_{n \to \infty} \left[\int_0^1 f_n \right] = \frac{1}{2} \]
\end{result}

\section{Uniform convergence}

\begin{result}
    {Uniform convergence of functions}
    {Definition}

    Let $f_n: E \rightarrow \mathbb{R}$ be a sequence of functions. 

    If $f$ is a function such that for all $\varepsilon$ there exists $N$ such that
    \[|f_n(x) - f(x)| \leq \varepsilon\]
    for all $x$, we say that $f$ converges \textit{uniformly}.
\end{result}

    This definition carries over to sums of functions (the partial sums must converge to the limit function uniformly).

This is a \textit{much stronger} notion of convergence, as it ``tethers'' together convergence of of all points in the domain. 

The following are useful criteria for uniform convergence. These hint at the idea of being able to make sense of the idea of ``distance'' between two functions, which Rudin makes precise later. The first one tells us that uniform convergence of functions corresponds to convergence (via the Cauchy criterion) with respect to a certain kind of metric. 

\begin{result}
    {Cauchy criterion for uniform convergence}
    {Theorem}

    $f_n$ converges uniformly on $E$ if and only if there exists an integer $N$ such that for all $m, n \geq N$,
    \[\sup_{x \in E}|f_n(x) - f_m(x)| \leq \varepsilon\]

    \textbf{Proof}

    For the forward implication, let $f_n \rightarrow f$ uniformly, then choose $N$ such that $n \geq N$ implies 
    \[|f_n(x) - f(x)| \leq \frac{\varepsilon}{2}\]
    Then use the triangle inequality.

    For the converse, we note that the above criterion is strong enough itself to guarantee $f_n \rightarrow f$ pointwise, hence if we take the inequality
    \[|f_n(x) - f_m(x)| \leq \varepsilon\]
    and let $m \rightarrow \infty$, we recover the original condition for uniform convergence
\end{result}

The next criterion is just a rephrasing of the first definition of uniform convergence. It tells us that uniform convergence corresponds to the ``distance'' between two functions vanishing.

\begin{result}
    {Supremum criterion for uniform convergence}
    {Theorem}

    $f_n \rightarrow f$ uniformly on $E$ if and only if 
    \[\sup_{x \in E}|f_n(x) - f(x)| \rightarrow 0 \:\text{ as }\: n \rightarrow \infty\]
\end{result}

Next, we show that uniform convergence does not share the same failures as pointwise convergence when it comes to passing through important properties and operations. We first take continuity.

\begin{result}
    {Continuity is preserved under uniform convergence}
    {Theorem}

    Let $f_n$ be a sequence of continuous functions and let $f_n \rightarrow f$ uniformly. Then $f$ is continuous.

    \textbf{Proof}

    Let $\varepsilon > 0$. Let $x \in E$. Consider the inequality
    \[|f(x) - f(t)| \leq |f(x) - f_n(x)| + |f_n(x) - f_n(t)| + |f_n(t) - f(t)|\]
First use uniform convergence to choose $n$ such that $|f(x) - f_n(x)|$ and $|f(t) - f_n(t)|$ are arbitrarily small. 

Then use continuity to choose $\delta$ such that $|f_n(x) - f_n(t)|$ can be made arbitrarily small by letting $|x-t| \leq \delta$.
\end{result}

This however is not an ``if and only if''. A continuous function can be the non-uniform limit of continuous functions. In one case, however, we can use compactness and monotonicity to take enough control such that a sequence of functions can \textit{only} converge uniformly.

\begin{result}
    {Inferring uniform convergence from pointwise convergence}
    {Example}
    
    Let $f_n: K \rightarrow \mathbb{R}$ be a sequence of continuous functions, and let $f_n \rightarrow f$ pointwise. Let $f$ be continuous. Impose the two conditions
    \begin{itemize}
        \item Let $K$ be compact
        \item Let $f_n(x) \geq f_{n+1}(x)$ for all $x$ and all $n$
    \end{itemize}
    Then $f_n \rightarrow f$ uniformly.

    \textbf{Proof}

    We must show that, as $n \rightarrow \infty$,
    \[\sup_{x \in E}|f_n(x) - f(x)| \rightarrow 0\]

    To do this, we choose $\varepsilon$ and argue that the set of points for which $|f_n(x) - f(x)| \geq \varepsilon$ can be made empty if we take $n$ large enough. Let this set of exceptional points be $K_n$.


    $K$'s compactness tells us that $K_n$ is compact:
    \begin{itemize}
        \item Because $f_n - f$ is a continuous function and $[\varepsilon, \infty)$ is closed, $K_n = (f_n-f)^{-1}([\varepsilon, \infty))$ is closed. (Theorem 4.8; inverse images of closed sets under continuous functions are closed) 
        \item Because $K_n$ is closed, and $K$ is compact, $K_n$ is compact. (Theorem 2.35; closed subsets of compact spaces are compact).
    \end{itemize}

    Moreover, because of the monotonicity of $f_n$, $K_n \supseteq K_{n+1}$. Hence $K_n$ is a sequence of nested compact sets.

    It must be that $\bigcap K_n$ is empty, as $f_n$ converges pointwise to $f$, and so for each $x$, the fact that $|f_n(x) - f| < \varepsilon$ for some $n$ means there must be some $K_n$ it is not a member of.

    But an intersection of nonempty nested compact sets must be nonempty (Theorem 2.36), so there has to be an empty set somewhere in the sequence. Let this set be $K_N$. Then for all $n \geq N$, $K_n$ is empty. This is precisely the $N$ we need to keep the distance between $f_n$ and $f$ below $\varepsilon$.
\end{result}

Restricting our study to continuous, bounded functions (the functions above were bounded because $K$ was compact) gives rise to a very nice structure, one we're already very familiar with: a metric space. (More importantly, a vector space!)

The concept of distance here corresponds with how it has been used in the previous theorems and examples.

\begin{result}
    {The space of continuous and bounded functions on $X$}
    {Definition}

    Let $X$ be a metric space. Let $\mathscr{C}(X)$ denote the set of all continuous, bounded, complex-valued functions on $X$.

    Let $f \in \mathscr{C}(X)$. Define the \textit{supremum norm} as follows.
    \[\Vert f \Vert = \sup_{x \in X} |f(x)|\]
    Let $d(f, g)$ for $f, g \in \mathscr{C}(X)$ be induced by the norm; i.e
    \[d(f, g) = \Vert f - g \Vert\]
    This turns $\mathscr{C}(X)$ into a metric space.
\end{result}

This space is complete, and the fact that it is so can be bootstrapped with the previous theorems.

\begin{result}
    {$\mathscr{C}(X)$ is complete}
    {Proof}

    Let $f_n$ be a Cauchy sequence in $\mathscr{C}(X)$. Then $f_n$ converges uniformly to a function $f$. Moreover, since each $f_n$ is continuous, $f$ is continuous. That $f$ is bounded follows from the fact that $f = f_n + (f - f_n)$, so $\Vert f \Vert \leq \Vert f_n \Vert + \Vert f - f_n \Vert = \Vert f_n \Vert + \varepsilon$. 

    Hence $f \in \mathscr{C}(X)$. Since $f_n \rightarrow f$ uniformly, $\Vert f - f_n \Vert \rightarrow 0$.
\end{result}


Next, we tackle integration. Uniform convergence allows us to bound $f$ in a, well, uniform way. This, along with a version of the squeeze theorem for integrals, shows us that uniform convergence plays nicely with integration.

\begin{result}
    {The integral of a uniformly convergent limit is a uniformly convergent limit of integrals}
    {Theorem}

    Let $f_n \in \mathscr{R}(\alpha)$ on $[a, b]$ for all $n$. Let $f_n \rightarrow f$ uniformly. Then $f \in \mathscr{R}(\alpha)$, and
    \[\int_a^b f d\alpha = \lim_{n \to \infty} \int_a^b f_n d\alpha\]

    \textbf{Proof}

    The idea is that you can ``squeeze'' $f$ with two $f_n$ shaped calipers, and that the gap decreases as $n$ goes to infinity.

    Let this gap be 
    \[\varepsilon_n = \Vert f_n - f\Vert\]
    Then
    \[f_n - \varepsilon_n \leq f \leq f_n + \varepsilon_n\]
    Then this bounds $f$'s lower and upper integrals.
    \[\int_a^b (f_n-\varepsilon_n) d\alpha \leq \underline{\int} f d\alpha \leq \overline{\int} f d\alpha \leq \int_a^b (f_n+\varepsilon_n) d\alpha\]
    Then
    \[0 \leq \overline{\int} f d\alpha - \underline{\int} fd\alpha \leq 2 \varepsilon(\alpha(b) - \alpha(a)) \]
    So $f \in \mathscr{R}(\alpha)$. From the inequality we can also read off 
    \[\left| \int_a^b fd\alpha - \int_a^b f_nd\alpha\right| \leq \varepsilon_n (\alpha(b) - \alpha(a))\]
    Which proves the equality of the integral and the limit.
\end{result}
    
Finally, we look at differentiation.

\begin{result}
    {A sequence of functions whose derivatives converge uniformly converges uniformly to a limit with the correct derivative}
    {Theorem}

    Let $f_n: [a,b] \rightarrow \mathbb{R}$ be a sequence of differentiable functions that converge pointwise for at least \textit{some} point $x_0$. Then if $f_n'$ converge uniformly, there is a function $f$ such that $f_n \rightarrow f$ uniformly and such that $f_n' \rightarrow f'$.

    \textbf{Proof}

    Let $\varepsilon > 0$. Use the Cauchy criterion for both ordinary sequences in $\mathbb{R}$ and for uniformly convergent sequences of functions to produce $N$ such that $n, m \geq N$ implies
    \[|f_n(x_0) - f_m(x_0)| \leq \frac{\varepsilon}{2}\]
    \[\Vert f'_n - f'_m \Vert \leq \frac{\varepsilon}{2(b-a)}\]
    Consider the function $f_n - f_m$, whose derivative is $f'_n - f'_m$. The mean value theorem gives us a $c$ with which we can show
    \[\left|\frac{[f_n(x) - f_m(x)] - [f_n(y) - f_m(y)]}{x-y}\right| = |f'_n(c) - f'_m(c)| \leq \Vert f'_n - f'_m \Vert \leq \frac{\varepsilon}{2(b-a)}\]
    Then, since $a \leq x < y \leq b$
    \[\left|[f_n(x) - f_m(x)] - [f_n(y) - f_m(y)]\right| \leq \frac{\varepsilon|x-y|}{2(b-a)} \leq \frac{\varepsilon}{2}\]
    for any $x, y \in [a, b]$. Then, by the triangle inequality,
    \[|f_n(x) - f_m(x)| \leq |[f_n(x) - f_m(x)] - [f_n(x_0) - f_m(x_0)]| + |f_n(x_0) - f_m(x_0)| \leq \varepsilon\]
    Then $f_n$ converges uniformly by the Cauchy criterion.

    Next we prove that the derivative of the limit is the limit of the derivatives. Let $f_n \rightarrow f$. Define the difference quotients
    \[\phi_n(t) := \frac{f_n(t) - f_n(x)}{t-x}\]
    \[\phi(t) := \frac{f(t) - f(x)}{t-x}\]
    Reusing the same inequality, we know that for $n, m \geq N$
    \[|\phi_n(t) - \phi_m(t)| \leq \frac{\varepsilon}{2(b-a)}\]
    So that $\phi_n \rightarrow \phi$ uniformly. Then, by Theorem 7.11, the limits we are interested in commute:
    \[\lim_{t \to x} \lim_{n \to \infty} \phi_n(t) = \lim_{n \to \infty} \lim_{t \to x} \phi_n(t)\]
    The left hand side is $f'(x)$
    \[\lim_{t \to x} \lim_{n \to \infty} \phi_n(t) = \lim_{t \to x} \phi(t) = f'(x)\]
    The right hand side is $\lim_{n\to\infty} f'_n(x)$
    \[\lim_{n \to \infty} \lim_{t \to x} \phi_n(t) = \lim_{n \to \infty} f'_n(x)\]
    Hence $f'_n \rightarrow f_n$.
\end{result}

Now with the machinery of limits of function sequences, we can explore a wider variety of functions. As an example.

\begin{result}
    {A nowhere differentiable continuous function}
    {Example}

    Define the periodic triangle function $\varphi$ to be
    \[\varphi(x) := |x|\]
    for $0 < x \leq 1$, and let its periodicity be defined by
    \[\varphi(x+2) := \varphi(x)\] 
    Then $\varphi$ is continuous. Define $f$ as
    \[f(x) := \sum_{n=0}^\infty \left(\frac{3}{4}\right)^n \varphi(4^nx)\]
    By Theorem 7.10, the series converges uniformly. Then $f$ is continuous, as it is a uniformly convergent sequence of continuous functions.
     To prove it is not differentiable, we construct a sequence $x_n \rightarrow x$ such that the difference quotient diverges. I haven't looked into it deeply yet.
 \end{result}


\section{Equicontinuous families of functions}

Now we go up another level. How do we \textit{produce} these uniformly convergent sequences of functions out of existing sequences? The same way we have conditions (boundedness in $\mathbb{R}^n$) for squeezing out convergent sequences of numbers? First consider two different kinds of boundedness.

\begin{result}
    {Pointwise and uniform boundedness}
    {Definition}
    
    Let $f_n: E \rightarrow \mathbb{R}$ be a sequence of functions.

    $f_n$ is \textit{pointwise bounded} if for some $\varphi: E \rightarrow \mathbb{R}$,
    \[|f_n(x)| < \varphi(x)\]
    $f_n$ is \textit{uniformly bounded} if for some $M \geq 0$,
    \[|f_n(x)| < M\]
    It is clear that uniform boundedness implies pointwise boundedness, by letting $\phi(x) := M$.
\end{result}

There are, however, no easy analogues, even when we put together all the restrictions we've considered so far:

\begin{result}
    {A uniformly bounded sequence of continuous functions on a compact set without a convergent subsequence}
    {Example}

    Let $f_n: [0, 2\pi] \to \mathbb{R}$ be defined 
    \[f_n(x) := \sin nx\]
    Suppose this sequence has a subsequence $f_{n_k}$ that converges pointwise. Then it must be that
    \[\lim_{k \to \infty} (\sin n_k x - \sin n_{k+1}x) = 0\]
    therefore it must be the case that
    \[\lim_{k \to \infty} (\sin n_k x - \sin n_{k+1}x)^2 = 0\]
    By Lebesgue's theorem concerning integration of boundedly convergent sequences (Theorem 11.32), this implies
    \[\lim_{k \to \infty} \int_0^{2\pi} (\sin n_k x - \sin n_{k+1} x)^2 dx = 0\]
    But 
    \[\int_0^{2\pi} (\sin n_k x - \sin n_{k+1} x)^2 dx = 2\pi\]
    which is a contradiction.
\end{result}

Even if we have a (pointwise) convergent sequence, we cannot produce a uniformly convergent subsequence necessarily, even with uniform boundedness on a compact set.

\begin{result}
    {A uniformly bounded pointwise converging sequence on a compact set without a uniformly convergent subsequence}
    {Example}

    Let $f_n: [0, 1] \rightarrow \mathbb{R}$ be
    \[f_n(x) = \frac{x^2}{x^2+(1-nx)^2}\]
    Then $|f_n| \leq 1$ for all $n$, and $f_n$ is a uniformly bounded sequence. $f_n(x) \to 0$ for all $x$ as $n \to \infty$ as well, so this sequence converges pointwise.
    However,
    \[f_n\left(\frac{1}{n}\right) = 1\]
    so that no subsequence can possibly converge uniformly.
\end{result}

However, we know that compact sets are separable (have a dense countable subset), hence the following consequence of pointwise boundedness will be useful:

\begin{result}
    {Every pointwise bounded sequence has a subsequence that converges pointwise on a countable set}
    {Theorem}

    Let $f_n$ be defined on a countable set $E$. (Or define it on an arbitrary set, and let $f_n$ be the restriction on a countable set $E$).

    Then there exists a subsequence $f_{n_k}$ of $f_n$ such that $f_{n_k}$ converges pointwise on $E$.

    \textbf{Proof}

    We will construct a table of $f_n$ of which taking the diagonal will give us the subsequence we want. To do so, we introduce a lemma

    \begin{myboxed}
    \textbf{Lemma}

    Let $s_n$ be a bounded sequence. Then it contains a convergent subsequence $s_{n_k}$ that may be obtained by simply deleting entries in $s_n$.
    \end{myboxed}

    This table will be defined row-by-row

    Let $x_i$ be an enumeration of $E$. Since $f_n$ is pointwise bounded, we can produce a subsequence $f_{n_k}$ such that $f_{n_k}(x_1)$ converges as $k \to \infty$. Denote this sequence by $f^1_k$. 

    Then in a similar fashion, we recursively construct the following sequences $f^{i+1}_k$ out of $f^i_k$ using the above lemma. This results in a table of functions
    \[\begin{matrix}
        f^1_1 & f^1_2 & f^1_3 & \cdots \\
        f^2_1 & f^2_2 & f^2_3 & \cdots \\
        f^3_1 & f^3_2 & f^3_3 & \cdots \\
        \vdots & \vdots & \vdots & \ddots
    \end{matrix}\]

    which satisfy the properties
    \begin{itemize}
        \item[(a)] $f^{i+1}_k$ is a subsequence of $f^i_k$, moreover it is simply $f^i_k$ with entries deleted. 
        \item[(b)] $f^i_k(x_i)$ converges as $k \to \infty$
    \end{itemize}

    Then, if we take the diagonal
    \[f^1_1, f^2_2,  f^3_3, \ldots\] 
    (a) implies that this sequence is a subsequence of all $f^i_k$, thus (b) implies that this sequence converges for all $x_i$.
\end{result}

Now we introduce an important property 

\begin{result}
    {Equicontinuous families of functions}
    {Definition}

    Let $\mathscr{F}$ be a family of complex-valued functions $f$ defined on a set $E$. $\mathscr{F}$ is \textit{equicontinuous} if for every $\varepsilon > 0$ there exists $\delta > 0$ such that
    \[|f(x) - f(y)| < \varepsilon\]
    whenever $d(x, y) < \delta$.
\end{result}

We get one result almost ``for free'' with this definition

\begin{result}
    {Uniformly convergent continuous function sequences on a compact domain form an equicontinuous family}
    {Theorem}

    Let $K$ be a compact metric space, and let $f_n \in \mathscr{C}(K)$ be a uniformly convergent sequence on $K$. Then $f_n$ is equicontinuous on $K$

    \textbf{Proof}

    Let $\varepsilon > 0$. Since $f_n$ converges in $\mathscr{C}(K)$, we can choose $N$ such that
    \[\Vert f_n - f_N \Vert < \varepsilon\]
    whenever $n > N$. Now, use the fact that continuous functions on compact sets are uniformly continuous (Theorem 4.19) to find $\delta_N > 0$ such that 
    \[|f_N(x) - f_N(y)| < \varepsilon\]
    whenever $d(x, y) < \delta_N$. This alone evidently covers the case $n = N$. For $n > N$, we have the following inequality,
    \[|f_n(x) - f_n(y)| \leq |f_n(x) - f_N(x)| + |f_N(x) - f_N(y)| + |f_N(y) - f_n(y)| < 3\varepsilon\]
    Hence we have shown that $\delta_N$ suffices for $n \geq N$. For $n < N$, use uniform continuity for each $f_n$ to get $\delta_n$ be such that $d(x, y) < \delta_n$ implies
    \[|f_i(x) - f_i(y)| < \varepsilon\]
    Finally, let $\delta = \min \{\delta_1, \ldots, \delta_N\}$. This $\delta$ satisfies the condition for all $n > 0$.
\end{result}

The next result uses the earlier theorem about pointwise converging subsequences on a countable set.

\begin{result}
    {A pointwise bounded and equicontinuous sequence of continuous functions on a compact domain is uniformly bounded and has a uniformly convergent subsequence}
    {Theorem}
    Let $K$ be a compact metric space. Let $f_n \in \mathscr{C}(K)$ be a pointwise bounded and equicontinuous sequence on $K$. Then
    \begin{itemize}
        \item[(a)] $f_n$ is uniformly bounded on $K$
        \item[(b)] $f_n$ contains a uniformly convergent subsequence
    \end{itemize}

    \textbf{Proof}

    Let $\varepsilon >0$ be given and choose $\delta > 0$. Since $f_n$ is equicontinuous, choose $\delta$ such that
    \[|f_n(x) - f_n(y)| < \varepsilon\]
    for all $n$ and $d(x, y) < \varepsilon$. Since $K$ is compact, we can cover it with finitely many $\delta$-balls centered at $p_1, \ldots p_r$. 

    Since $f_n$ is pointwise convergent, we may bound each $|f_n(p_i)|$ by some $\varphi(p_i)$. Since we have only finitely many $p_i$, we may take $M := \max \{\varphi(p_1), \ldots, \varphi(p_r)\}$Then $|f_n(p_i)| < M$ for all $i$, hence $|f_n(x)| < M + \varepsilon$ for all $x \in K$ by the triangle inequality. Hence $f_n$ is uniformly bounded.

    Now, for (b), let $E$ be a countable dense subset of $K$. (Existence is guaranteed-- compact metric spaces are separable)

    Now use the previous theorem that tells us there is a subsequence $f_{n_k}$ that converges pointwise on $E$.

    Denote this subsequence by $g_k := f_{n_k}$. Now let $\varepsilon > 0$ and $\delta > 0$ be the same as before. Since $E$ is dense and $K$ is compact, we may cover $K$ with finitely many open balls $B(x_i; \delta)$ centered at points $x_1, \ldots, x_m$ of radius $\delta$. In other words,
    \[K \subset B(x_1; \delta) \cup \cdots \cup B(x_m; \delta)\]
    Then we can use $x_i$ as a bridge to connect $g_j(x)$ and $g_k(x)$: By the pointwise convergence of $g_k$, we know that we can choose $N$ such that $j, k \geq N$ implies 
    \[|g_j(x_i) - g_k(x_i)| < \varepsilon\]
    Let $x$ belong to $B(x_i; \delta)$, then
    \[|g_j(x) - g_j(x_i)| < \varepsilon\]
    Then we construct a big triangle inequality:
    \[|g_j(x) - g_k(x)| \leq |g_j(x) - g_j(x_i)| + |g_j(x_i) - g_k(x_i)| + |g_k(x_i) - g_k(x)| < 3\varepsilon\]
    Which proves that $\Vert g_j - g_k \Vert < 3\varepsilon$, hence $g_k$ is uniformly convergent.
\end{result}

WOOHOO!

Now we approach the big theorem this chapter is leading up to: the \textit{Stone-Weierstrass Theorem}.

\begin{result}
    {The Stone-Weierstrass Theorem}
    {Theorem}

    Let $f: [a, b] \to \mathbb{C}$ be a continuous function. Then there exists a sequence of polynomials $P_n$ such that $P_n \to f$ uniformly on $[a, b]$. Moreover, if $f$ is real, then each $P_n$ is real.
\end{result}

First, some housekeeping is in order.

\begin{result}
    {Proof of Stone-Weierstrass, Part 0: Setup}
    {Theorem}

    It suffices to prove the theorem for functions $f \in \mathscr{C}([a, b])$ such that 
    \begin{itemize}
        \item[(a)] $[a, b] = [0,1]$
        \item[(b)] $f(0) = f(1) = 0$
    \end{itemize}
    to prove the theorem in general.

    \textbf{Proof}

     The first is justified via horizontal shifts and dilations, the second is justified by shears on the $y$ axis. 
\end{result}

Then, we start the proof

\begin{result}
    {Proof of Stone-Weierstrass, Part 1: Strategy}
    {Outline}

    We use \textit{convolution}, which is a tool that analyzes a function $f$ by multiplying it with a function $g$ at the point of interest. 

    One use of convolution is to isolate segments of $f$ by using a $g$ that is zero, or very close to zero, outside a neighborhood of the point we wish to study. An example of this would be using very tall Gaussians. Moreover, if the integral of this $g$ is $1$, the value of the convolution integral will almost be $f(x)$ itself!

    Luckily, there is a sequence of polynomials $Q_n$ which do this task well. More luckily, the convolution of \textit{any function at all} with a polynomial results in a polynomial! This gives us an on-the-nose way of \textit{constructing} the polynomials we want-- taking the convolution of $f$ and $Q_n$.
    
    Finally, we show that this sequence really works-- that it really does converge to $f$ in the uniform limit.
\end{result}

We have to define convolution

\begin{result}
    {Convolution}
    {Definition}

    Let $f \in \mathscr{C}([a,b])$ and $g \in \mathscr{C}([c, d])$. Assume that $f$ and $g$ are defined to be $0$ outside their domains. The \textit{convolution} $f \ast g$ is defined to be
    \[(f \ast g)(x) := \int_a^b f(t)g(x-t) dt\]
    This is guaranteed to exist, by Theorem 6.13. 
\end{result}

\begin{result}
    {Proof of Stone-Weierstrass, Part 2: Constructing the sequence}
    {Theorem}

    Define the polynomial $Q_n := c_n(1-x^2)^n$, where $c_n$ is chosen such that
    \[\int_{-1}^1 Q_n(x)dx = 1\]

    Then define
    \[P_n := f \ast Q_n\]

    Then $P_n$ is a sequence of polynomials.

    \textbf{Proof}

    \begin{result}
        {Convolution with a polynomial is a polynomial}
        {Lemma}

        Let $P \in \mathscr{C}([-1, 1])$ be a polynomial, and $f \in \mathscr{C}([0, 1])$, then $f \ast P$ is a polynomial.
        \textbf{Proof}

        By linearity of convolution, we can consider the case $P(x) := x^n$ without loss of generality. Then
        \[(f \ast P) = \int_0^1 f(t)(x-t)^ndt = \int_0^1 f(t) \left[\sum_{k=0}^n \binom{n}{k}x^{n}(-t)^{n-k}\right]dt\]
        \[= \sum_{k=0}^n x^n \left[\binom{n}{k}\int_0^1 f(t) (-t)^{n-k}dt\right]\]
        \[= \sum_{k=0}^n x^n \left[(-1)^{n-k}\binom{n}{k}\int_0^1 f(t)t^{n-k}dt\right]\]
        Which is evidently a polynomial. The coefficient is guaranteed to exist, as $f(t) \cdot t^{n-k}$ is continuous on $[a, b]$, hence the integral in the brackets is defined.
    \end{result}

\end{result}


\begin{result}
    {Proof of Stone-Weierstrass, Part 3: Convergence}
    {Theorem}

    $P_n \rightarrow f$ uniformly on $[0, 1]$.

    \textbf{Proof}
    
    By symmetry of convolution (change of variables), we have that
    \[P_n = \int_{-1}^1 Q_n(t)f(x-t)dt\]
    Then,
    \[|P_n(x) - f(x)| = \left|\int_{-1}^1 Q_n(t) f(x-t) - f(x)\right|\] 
    Since $\int_{-1}^1 Q_n(t)dt = 1$,
\[|P_n(x) - f(x)| =\left|\int_{-1}^1 Q_n(t)f(x-t) dt - \left[\int_{-1}^1 Q_n(t)dt\right]f(x)\right|\]
Hence
\[|P_n(x) - f(x)| =\left|\int_{-1}^1 Q_n(t)[f(x-t)-f(x)] dt\right|\]
Using the Schwarz inequality and the fact that $Q_n$ is nonnegative, 
\[|P_n(x) - f(x)| \leq \int_{-1}^1 Q_n(t)\big|f(x-t)-f(x)\big| dt\]
The idea now is to divide this integral, which measures the error of the polynomial approximation, into a ``local'' and a ``non-local'' part. The local error's can be bounded by $f$'s uniform continuity, the non-local part can be bounded by $Q_n$ shrinking outside of $t$.

Now, choose $\varepsilon > 0$. We can find $\delta$ such that 
\[|f(y) - f(x)| < \varepsilon/2\]
for all $x, y$ such that $|y-x| < \delta$. This tells us that 
\[|f(x-t) - f(x)| < \varepsilon/2\]
for all $t \in (-\delta, \delta)$. Hence
\[\int_{-\delta}^\delta |f(x-t) - f(x)| Q_n(t) dt < \frac{\varepsilon}{2}\int_{-\delta}^\delta Q_n(t) < \frac{\varepsilon}{2}\]

This is our ``local" error. Our ``non-local'' error is then
\[\int_{-1}^{-\delta} |f(x-t)-f(x)|Q_n(t)dt + \int_{\delta}^1 |f(x-t)-f(x)|Q_n(t)dt\]
\end{result}

\begin{myboxed}
    The book uses the inequality $c_n < \sqrt{n}$ to derive
    \[Q_n(x) \leq \sqrt{n}(1-\delta^2)^n\]
    Which we will just use at face-value because life is too short. Letting $M = \sup |f(x)|$, we have that $|f(x-t) - f(x)| \leq 2M$, hence, taking it all together, 
\[\int_{-1}^{-\delta} |f(x-t)-f(x)|Q_n(t)dt + \int_{\delta}^1 |f(x-t)-f(x)|Q_n(t)dt\]
\[\leq 2M \int_{-1}^{-\delta}Q_n(t)dt + 2M \int_{\delta}^{1}Q_n(t)dt\]
\[\leq 4M \sqrt{n}(1-\delta^2)^n\]
Which can be made to be less than $\varepsilon/2$ by making $n$ large enough.
\end{myboxed}
